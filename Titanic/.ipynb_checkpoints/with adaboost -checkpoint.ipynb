{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "\n",
    "train = pd.read_csv('train.csv', header = 0, dtype={'Age': np.float64})\n",
    "test  = pd.read_csv('test.csv' , header = 0, dtype={'Age': np.float64})\n",
    "full_data = [train, test]\n",
    "\n",
    "print (train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0.472826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0.242363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Survived\n",
       "Pclass          \n",
       "1       0.629630\n",
       "2       0.472826\n",
       "3       0.242363"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"Pclass\", \"Survived\"]].groupby(\"Pclass\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td> 0.742038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td> 0.188908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Survived\n",
       "Sex             \n",
       "female  0.742038\n",
       "male    0.188908"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"Sex\", \"Survived\"]].groupby(\"Sex\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parch and SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare\n",
    "Fill the missing values with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "dtypes: float64(2), int64(7), object(5)\n",
      "memory usage: 104.4+ KB\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "#     dataset['Fare'] = dataset['Fare'][dataset['Fare']].astype(int)\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'][train['Fare'].notnull()].median())\n",
    "    \n",
    "#     dataset['Fare'] = dataset['Fare'].astype('int64')\n",
    "# train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "# print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\n",
    "\n",
    "train.columns\n",
    "train.info()\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Age\"][train[\"Age\"].isnull()].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many missing values for age so we can fill it with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null int64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "dtypes: float64(1), int64(8), object(5)\n",
      "memory usage: 104.4+ KB\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg = dataset[\"Age\"][dataset[\"Age\"].notnull()].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "\n",
    "    age_rand_list = np.random.randint(age_avg-age_std, age_avg+age_std, size=age_null_count)\n",
    "    dataset[\"Age\"][dataset[\"Age\"].isnull()] = age_rand_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "#     dataset[\"CategoricalAge\"] = pd.qcut(dataset[\"Age\"], 5)\n",
    "# #     print(age_null_count)\n",
    "# print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex       female  male\n",
      "Title                 \n",
      "Capt           0     1\n",
      "Col            0     2\n",
      "Countess       1     0\n",
      "Don            0     1\n",
      "Dr             1     6\n",
      "Jonkheer       0     1\n",
      "Lady           1     0\n",
      "Major          0     2\n",
      "Master         0    40\n",
      "Miss         182     0\n",
      "Mlle           2     0\n",
      "Mme            1     0\n",
      "Mr             0   517\n",
      "Mrs          125     0\n",
      "Ms             1     0\n",
      "Rev            0     6\n",
      "Sir            0     1\n"
     ]
    }
   ],
   "source": [
    "import re as re\n",
    "def get_title(name):\n",
    "\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "\t# If the title exists, extract and return it.\n",
    "\tif title_search:\n",
    "\t\treturn title_search.group(1)\n",
    "\treturn \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "print(pd.crosstab(train['Title'], train['Sex']))\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null int64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "Title          891 non-null object\n",
      "dtypes: float64(1), int64(8), object(6)\n",
      "memory usage: 111.4+ KB\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'FamilySize', 'IsAlone', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null int64\n",
      "Age            891 non-null int64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null int64\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "Title          891 non-null int64\n",
      "dtypes: float64(1), int64(11), object(3)\n",
      "memory usage: 111.4+ KB\n",
      "   Survived  Pclass  Sex  Age     Fare  Embarked  IsAlone  Title\n",
      "0         0       3    1   22   7.2500         0        0      1\n",
      "1         1       1    0   38  71.2833         1        0      3\n",
      "2         1       3    0   26   7.9250         0        1      2\n",
      "3         1       1    0   35  53.1000         0        0      3\n",
      "4         0       3    1   35   8.0500         0        1      1\n",
      "5         0       3    1   34   8.4583         2        1      1\n",
      "6         0       1    1   54  51.8625         0        1      1\n",
      "7         0       3    1    2  21.0750         0        0      4\n",
      "8         1       3    0   27  11.1333         0        0      3\n",
      "9         1       2    0   14  30.0708         1        0      3\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null int64\n",
      "Age         891 non-null int64\n",
      "Fare        891 non-null float64\n",
      "Embarked    891 non-null int64\n",
      "IsAlone     891 non-null int64\n",
      "Title       891 non-null int64\n",
      "dtypes: float64(1), int64(7)\n",
      "memory usage: 62.6 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in full_data:\n",
    "#     # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} )\n",
    "\n",
    "    #     # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    #     # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} )\n",
    "    \n",
    "#     # Mapping Fare\n",
    "#     dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "#     dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "#     dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "#     dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "#     dataset['Fare'] = dataset['Fare']\n",
    "    \n",
    "#     # Mapping Age\n",
    "#     dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "#     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "#     dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "#     dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "#     dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n",
    "\n",
    "# Feature Selection\n",
    "train.info()\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n",
    "                 'Parch', 'FamilySize']\n",
    "traindr = train.drop(drop_elements, axis = 1)\n",
    "# train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "testdr  = test.drop(drop_elements, axis = 1)\n",
    "\n",
    "print (traindr.head(10))\n",
    "\n",
    "traind = traindr.values\n",
    "testd  = testdr.values\n",
    "traindr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X = traindr[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'IsAlone', 'Title']].values\n",
    "y = traindr['Survived'].values\n",
    "X_final = testdr[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'IsAlone', 'Title']].values\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ss = StandardScaler(copy=False)\n",
    "# ss.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 7)\n",
      "(801,)\n",
      "(90, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "[X_train, X_test, y_train, y_test] = train_test_split(X, y, test_size=0.1, random_state=23)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "# out = pd.DataFrame(data=X_test)\n",
    "# out.to_csv(path_or_buf='testout.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87777777777777777"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1, n_jobs=-1)\n",
    "clf2 = SVC(random_state=1, probability=True)\n",
    "clf3 = KNeighborsClassifier( n_jobs=-1)\n",
    "clf4 = RandomForestClassifier(n_jobs=-1, n_estimators=1000, criterion='gini', max_depth=5)\n",
    "\n",
    "pipeline1 = Pipeline([('ssc1', StandardScaler()),\n",
    "                      ('lr', clf1)])\n",
    "pipeline2 = Pipeline([('ssc2', StandardScaler()),\n",
    "                      ('lr', clf2)])\n",
    "pipeline3 = Pipeline([('ssc3', StandardScaler()),\n",
    "                      ('lr', clf3)])\n",
    "\n",
    "estimators = [('eclf1', pipeline1), ('eclf2', pipeline2), ('eclf3', pipeline3), ('eclf4', clf4)]\n",
    "\n",
    "vc = VotingClassifier(estimators=estimators, n_jobs=-1)\n",
    "\n",
    "vc.fit(X_train, y_train)\n",
    "vc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.888888888889\n",
      "0.822222222222\n",
      "0.855555555556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline1.fit(X_train,y_train)\n",
    "print(pipeline1.score(X_test, y_test))\n",
    "pipeline2.fit(X_train,y_train)\n",
    "print(pipeline2.score(X_test, y_test))\n",
    "pipeline3.fit(X_train,y_train)\n",
    "print(pipeline3.score(X_test, y_test))\n",
    "clf4.fit(X_train,y_train)\n",
    "print(clf4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eclf1': Pipeline(steps=[('ssc1', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "           penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " 'eclf1__lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "           penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'eclf1__lr__C': 1.0,\n",
       " 'eclf1__lr__class_weight': None,\n",
       " 'eclf1__lr__dual': False,\n",
       " 'eclf1__lr__fit_intercept': True,\n",
       " 'eclf1__lr__intercept_scaling': 1,\n",
       " 'eclf1__lr__max_iter': 100,\n",
       " 'eclf1__lr__multi_class': 'ovr',\n",
       " 'eclf1__lr__n_jobs': -1,\n",
       " 'eclf1__lr__penalty': 'l2',\n",
       " 'eclf1__lr__random_state': 1,\n",
       " 'eclf1__lr__solver': 'liblinear',\n",
       " 'eclf1__lr__tol': 0.0001,\n",
       " 'eclf1__lr__verbose': 0,\n",
       " 'eclf1__lr__warm_start': False,\n",
       " 'eclf1__ssc1': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'eclf1__ssc1__copy': True,\n",
       " 'eclf1__ssc1__with_mean': True,\n",
       " 'eclf1__ssc1__with_std': True,\n",
       " 'eclf1__steps': [('ssc1',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('lr',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))],\n",
       " 'eclf2': Pipeline(steps=[('ssc2', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001,\n",
       "   verbose=False))]),\n",
       " 'eclf2__lr': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001,\n",
       "   verbose=False),\n",
       " 'eclf2__lr__C': 1.0,\n",
       " 'eclf2__lr__cache_size': 200,\n",
       " 'eclf2__lr__class_weight': None,\n",
       " 'eclf2__lr__coef0': 0.0,\n",
       " 'eclf2__lr__decision_function_shape': None,\n",
       " 'eclf2__lr__degree': 3,\n",
       " 'eclf2__lr__gamma': 'auto',\n",
       " 'eclf2__lr__kernel': 'rbf',\n",
       " 'eclf2__lr__max_iter': -1,\n",
       " 'eclf2__lr__probability': True,\n",
       " 'eclf2__lr__random_state': 1,\n",
       " 'eclf2__lr__shrinking': True,\n",
       " 'eclf2__lr__tol': 0.001,\n",
       " 'eclf2__lr__verbose': False,\n",
       " 'eclf2__ssc2': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'eclf2__ssc2__copy': True,\n",
       " 'eclf2__ssc2__with_mean': True,\n",
       " 'eclf2__ssc2__with_std': True,\n",
       " 'eclf2__steps': [('ssc2',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('lr', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "     max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001,\n",
       "     verbose=False))],\n",
       " 'eclf3': Pipeline(steps=[('ssc3', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "            weights='uniform'))]),\n",
       " 'eclf3__lr': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       " 'eclf3__lr__algorithm': 'auto',\n",
       " 'eclf3__lr__leaf_size': 30,\n",
       " 'eclf3__lr__metric': 'minkowski',\n",
       " 'eclf3__lr__metric_params': None,\n",
       " 'eclf3__lr__n_jobs': -1,\n",
       " 'eclf3__lr__n_neighbors': 5,\n",
       " 'eclf3__lr__p': 2,\n",
       " 'eclf3__lr__weights': 'uniform',\n",
       " 'eclf3__ssc3': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'eclf3__ssc3__copy': True,\n",
       " 'eclf3__ssc3__with_mean': True,\n",
       " 'eclf3__ssc3__with_std': True,\n",
       " 'eclf3__steps': [('ssc3',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('lr',\n",
       "   KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "              weights='uniform'))],\n",
       " 'eclf4': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "             random_state=None, verbose=0, warm_start=False),\n",
       " 'eclf4__bootstrap': True,\n",
       " 'eclf4__class_weight': None,\n",
       " 'eclf4__criterion': 'gini',\n",
       " 'eclf4__max_depth': 5,\n",
       " 'eclf4__max_features': 'auto',\n",
       " 'eclf4__max_leaf_nodes': None,\n",
       " 'eclf4__min_impurity_split': 1e-07,\n",
       " 'eclf4__min_samples_leaf': 1,\n",
       " 'eclf4__min_samples_split': 2,\n",
       " 'eclf4__min_weight_fraction_leaf': 0.0,\n",
       " 'eclf4__n_estimators': 1000,\n",
       " 'eclf4__n_jobs': -1,\n",
       " 'eclf4__oob_score': False,\n",
       " 'eclf4__random_state': None,\n",
       " 'eclf4__verbose': 0,\n",
       " 'eclf4__warm_start': False,\n",
       " 'estimators': [('eclf1',\n",
       "   Pipeline(steps=[('ssc1', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "             penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))])),\n",
       "  ('eclf2',\n",
       "   Pipeline(steps=[('ssc2', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "     max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001,\n",
       "     verbose=False))])),\n",
       "  ('eclf3',\n",
       "   Pipeline(steps=[('ssc3', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "              weights='uniform'))])),\n",
       "  ('eclf4',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "               random_state=None, verbose=0, warm_start=False))],\n",
       " 'n_jobs': -1,\n",
       " 'voting': 'hard',\n",
       " 'weights': None}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   23.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826466916355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'eclf1__lr__C': [ 0.01],\n",
    "    'eclf1__lr__penalty': ['l2'],\n",
    "    'eclf2__lr__C': [1.0],\n",
    "    'voting': ['soft'],\n",
    "    'eclf4__max_depth': [4,5,6,None],\n",
    "    'eclf3__lr__n_neighbors':[5],\n",
    "    \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=vc, param_grid=param_grid, cv=10, verbose=1, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826466916355\n",
      "{'eclf1__lr__C': 0.01, 'eclf1__lr__penalty': 'l2', 'eclf2__lr__C': 1.0, 'eclf3__lr__n_neighbors': 5, 'eclf4__max_depth': None, 'voting': 'soft'}\n",
      "VotingClassifier(estimators=[('eclf1', Pipeline(steps=[('ssc1', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_sta...tors=1000, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False))],\n",
      "         n_jobs=-1, voting='soft', weights=None)\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877777777778\n",
      "(418, 7)\n"
     ]
    }
   ],
   "source": [
    "est = gs.best_estimator_\n",
    "print(est.score(X_test,y_test))\n",
    "est.fit(X,y)\n",
    "# vc.fit(X_train, y_train)\n",
    "# print(vc.score(X_test,y_test))\n",
    "# vc.fit(X,y)\n",
    "\n",
    "print(X_final.shape)\n",
    "# y_pred_teat = vc.predict(X_final)\n",
    "y_pred_teat = est.predict(X_final)\n",
    "stest  = pd.read_csv('test.csv' , header = 0, dtype={'Age': np.float64})\n",
    "s = np.vstack((stest['PassengerId'].values, y_pred_teat))\n",
    "# s\n",
    "\n",
    "submit = pd.DataFrame(data =s.transpose(), columns=(\"PassengerId\", \"Survived\"))\n",
    "submit\n",
    "submit.to_csv(path_or_buf='ensembleGridSearch2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
