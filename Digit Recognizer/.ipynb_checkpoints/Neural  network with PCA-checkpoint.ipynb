{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline  \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 20212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000,785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data_t = pd.read_csv('test.csv')\n",
    "\n",
    "print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images(42000,784)\n",
      "images_t(28000,784)\n"
     ]
    }
   ],
   "source": [
    "images = data.iloc[:,1:].values\n",
    "images_t = data_t.iloc[:,:].values \n",
    "images = images.astype(np.float)\n",
    "images_t = images_t.astype(np.float)\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "images_t = np.multiply(images_t, 1.0 / 255.0)\n",
    "print('images({0[0]},{0[1]})'.format(images.shape))\n",
    "print('images_t({0[0]},{0[1]})'.format(images_t.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d0bfa6e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display image\n",
    "def display(img):\n",
    "    \n",
    "    # (784) => (28,28)\n",
    "    one_image = img.reshape(28,28)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n",
    "\n",
    "# output image     \n",
    "display(images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    }
   ],
   "source": [
    "X = images\n",
    "y = data.iloc[:,0].values\n",
    "print(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((42000, 100), 0.0065595519315426462)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=100,svd_solver ='full', whiten = True)\n",
    "pca.fit(X)\n",
    "X_ = pca.transform(X)\n",
    "print(X_.shape, pca.noise_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d0cdecf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# output image\n",
    "test = pca.inverse_transform(X_[10])\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82982361\n",
      "Iteration 2, loss = 0.48327908\n",
      "Iteration 3, loss = 0.33798879\n",
      "Iteration 4, loss = 0.27971064\n",
      "Iteration 5, loss = 0.24113217\n",
      "Iteration 6, loss = 0.21310501\n",
      "Iteration 7, loss = 0.19095748\n",
      "Iteration 8, loss = 0.17336554\n",
      "Iteration 9, loss = 0.15861888\n",
      "Iteration 10, loss = 0.14628828\n",
      "Iteration 11, loss = 0.13597700\n",
      "Iteration 12, loss = 0.12617875\n",
      "Iteration 13, loss = 0.11818810\n",
      "Iteration 14, loss = 0.11073158\n",
      "Iteration 15, loss = 0.10420493\n",
      "Iteration 16, loss = 0.09859914\n",
      "Iteration 17, loss = 0.09289349\n",
      "Iteration 18, loss = 0.08770030\n",
      "Iteration 19, loss = 0.08376428\n",
      "Iteration 20, loss = 0.07910345\n",
      "Iteration 21, loss = 0.07506046\n",
      "Iteration 22, loss = 0.07204661\n",
      "Iteration 23, loss = 0.06852400\n",
      "Iteration 24, loss = 0.06525537\n",
      "Iteration 25, loss = 0.06247766\n",
      "Iteration 26, loss = 0.05957101\n",
      "Iteration 27, loss = 0.05697548\n",
      "Iteration 28, loss = 0.05436399\n",
      "Iteration 29, loss = 0.05207830\n",
      "Iteration 30, loss = 0.04997802\n",
      "Iteration 31, loss = 0.04784019\n",
      "Iteration 32, loss = 0.04615294\n",
      "Iteration 33, loss = 0.04439975\n",
      "Iteration 34, loss = 0.04287394\n",
      "Iteration 35, loss = 0.04099662\n",
      "Iteration 36, loss = 0.03967887\n",
      "Iteration 37, loss = 0.03810813\n",
      "Iteration 38, loss = 0.03699541\n",
      "Iteration 39, loss = 0.03573192\n",
      "Iteration 40, loss = 0.03450250\n",
      "Iteration 41, loss = 0.03309071\n",
      "Iteration 42, loss = 0.03213823\n",
      "Iteration 43, loss = 0.03112547\n",
      "Iteration 44, loss = 0.03044175\n",
      "Iteration 45, loss = 0.02949230\n",
      "Iteration 46, loss = 0.02832033\n",
      "Iteration 47, loss = 0.02760431\n",
      "Iteration 48, loss = 0.02679509\n",
      "Iteration 49, loss = 0.02630744\n",
      "Iteration 50, loss = 0.02538020\n",
      "Iteration 51, loss = 0.02504598\n",
      "Iteration 52, loss = 0.02419189\n",
      "Iteration 53, loss = 0.02352272\n",
      "Iteration 54, loss = 0.02307416\n",
      "Iteration 55, loss = 0.02249033\n",
      "Iteration 56, loss = 0.02199173\n",
      "Iteration 57, loss = 0.02140629\n",
      "Iteration 58, loss = 0.02101198\n",
      "Iteration 59, loss = 0.02062676\n",
      "Iteration 60, loss = 0.02021296\n",
      "Iteration 61, loss = 0.01992266\n",
      "Iteration 62, loss = 0.01938454\n",
      "Iteration 63, loss = 0.01906628\n",
      "Iteration 64, loss = 0.01878556\n",
      "Iteration 65, loss = 0.01841092\n",
      "Iteration 66, loss = 0.01809671\n",
      "Iteration 67, loss = 0.01787390\n",
      "Iteration 68, loss = 0.01751059\n",
      "Iteration 69, loss = 0.01726204\n",
      "Iteration 70, loss = 0.01708148\n",
      "Iteration 71, loss = 0.01685839\n",
      "Iteration 72, loss = 0.01668863\n",
      "Iteration 73, loss = 0.01643110\n",
      "Iteration 74, loss = 0.01624134\n",
      "Iteration 75, loss = 0.01607756\n",
      "Iteration 76, loss = 0.01590650\n",
      "Iteration 77, loss = 0.01567395\n",
      "Iteration 78, loss = 0.01558676\n",
      "Iteration 79, loss = 0.01542264\n",
      "Iteration 80, loss = 0.01525480\n",
      "Iteration 81, loss = 0.01511494\n",
      "Iteration 82, loss = 0.01503345\n",
      "Iteration 83, loss = 0.01487967\n",
      "Iteration 84, loss = 0.01479124\n",
      "Iteration 85, loss = 0.01463468\n",
      "Iteration 86, loss = 0.01459449\n",
      "Iteration 87, loss = 0.01445135\n",
      "Iteration 88, loss = 0.01431828\n",
      "Iteration 89, loss = 0.01429542\n",
      "Iteration 90, loss = 0.01414846\n",
      "Iteration 91, loss = 0.01408557\n",
      "Iteration 92, loss = 0.01399914\n",
      "Iteration 93, loss = 0.01392759\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83170397\n",
      "Iteration 2, loss = 0.48201443\n",
      "Iteration 3, loss = 0.33559312\n",
      "Iteration 4, loss = 0.27845200\n",
      "Iteration 5, loss = 0.23979313\n",
      "Iteration 6, loss = 0.21238224\n",
      "Iteration 7, loss = 0.19176323\n",
      "Iteration 8, loss = 0.17397517\n",
      "Iteration 9, loss = 0.15858376\n",
      "Iteration 10, loss = 0.14578923\n",
      "Iteration 11, loss = 0.13510082\n",
      "Iteration 12, loss = 0.12584334\n",
      "Iteration 13, loss = 0.11759330\n",
      "Iteration 14, loss = 0.10995982\n",
      "Iteration 15, loss = 0.10334748\n",
      "Iteration 16, loss = 0.09711533\n",
      "Iteration 17, loss = 0.09119565\n",
      "Iteration 18, loss = 0.08649825\n",
      "Iteration 19, loss = 0.08155734\n",
      "Iteration 20, loss = 0.07727503\n",
      "Iteration 21, loss = 0.07295885\n",
      "Iteration 22, loss = 0.06918263\n",
      "Iteration 23, loss = 0.06619872\n",
      "Iteration 24, loss = 0.06283335\n",
      "Iteration 25, loss = 0.06003943\n",
      "Iteration 26, loss = 0.05746698\n",
      "Iteration 27, loss = 0.05500381\n",
      "Iteration 28, loss = 0.05232641\n",
      "Iteration 29, loss = 0.05058874\n",
      "Iteration 30, loss = 0.04824867\n",
      "Iteration 31, loss = 0.04603087\n",
      "Iteration 32, loss = 0.04443692\n",
      "Iteration 33, loss = 0.04278696\n",
      "Iteration 34, loss = 0.04104860\n",
      "Iteration 35, loss = 0.03917356\n",
      "Iteration 36, loss = 0.03797706\n",
      "Iteration 37, loss = 0.03657836\n",
      "Iteration 38, loss = 0.03532402\n",
      "Iteration 39, loss = 0.03425749\n",
      "Iteration 40, loss = 0.03278991\n",
      "Iteration 41, loss = 0.03174210\n",
      "Iteration 42, loss = 0.03068677\n",
      "Iteration 43, loss = 0.02969462\n",
      "Iteration 44, loss = 0.02887867\n",
      "Iteration 45, loss = 0.02783387\n",
      "Iteration 46, loss = 0.02689859\n",
      "Iteration 47, loss = 0.02598196\n",
      "Iteration 48, loss = 0.02526494\n",
      "Iteration 49, loss = 0.02482854\n",
      "Iteration 50, loss = 0.02410741\n",
      "Iteration 51, loss = 0.02369353\n",
      "Iteration 52, loss = 0.02289730\n",
      "Iteration 53, loss = 0.02248064\n",
      "Iteration 54, loss = 0.02185031\n",
      "Iteration 55, loss = 0.02140343\n",
      "Iteration 56, loss = 0.02084852\n",
      "Iteration 57, loss = 0.02037629\n",
      "Iteration 58, loss = 0.01999318\n",
      "Iteration 59, loss = 0.01962044\n",
      "Iteration 60, loss = 0.01923916\n",
      "Iteration 61, loss = 0.01884145\n",
      "Iteration 62, loss = 0.01864667\n",
      "Iteration 63, loss = 0.01827956\n",
      "Iteration 64, loss = 0.01789505\n",
      "Iteration 65, loss = 0.01766691\n",
      "Iteration 66, loss = 0.01733774\n",
      "Iteration 67, loss = 0.01715150\n",
      "Iteration 68, loss = 0.01678362\n",
      "Iteration 69, loss = 0.01660394\n",
      "Iteration 70, loss = 0.01640690\n",
      "Iteration 71, loss = 0.01617808\n",
      "Iteration 72, loss = 0.01594266\n",
      "Iteration 73, loss = 0.01575642\n",
      "Iteration 74, loss = 0.01553100\n",
      "Iteration 75, loss = 0.01537652\n",
      "Iteration 76, loss = 0.01527086\n",
      "Iteration 77, loss = 0.01507720\n",
      "Iteration 78, loss = 0.01490932\n",
      "Iteration 79, loss = 0.01474134\n",
      "Iteration 80, loss = 0.01467375\n",
      "Iteration 81, loss = 0.01448835\n",
      "Iteration 82, loss = 0.01438207\n",
      "Iteration 83, loss = 0.01424994\n",
      "Iteration 84, loss = 0.01416816\n",
      "Iteration 85, loss = 0.01405546\n",
      "Iteration 86, loss = 0.01395500\n",
      "Iteration 87, loss = 0.01386353\n",
      "Iteration 88, loss = 0.01373263\n",
      "Iteration 89, loss = 0.01362511\n",
      "Iteration 90, loss = 0.01358538\n",
      "Iteration 91, loss = 0.01345053\n",
      "Iteration 92, loss = 0.01342253\n",
      "Iteration 93, loss = 0.01334883\n",
      "Iteration 94, loss = 0.01326541\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82962547\n",
      "Iteration 2, loss = 0.48278674\n",
      "Iteration 3, loss = 0.33977416\n",
      "Iteration 4, loss = 0.28000395\n",
      "Iteration 5, loss = 0.24165165\n",
      "Iteration 6, loss = 0.21353286\n",
      "Iteration 7, loss = 0.19101907\n",
      "Iteration 8, loss = 0.17354228\n",
      "Iteration 9, loss = 0.15941468\n",
      "Iteration 10, loss = 0.14652508\n",
      "Iteration 11, loss = 0.13520083\n",
      "Iteration 12, loss = 0.12594581\n",
      "Iteration 13, loss = 0.11753177\n",
      "Iteration 14, loss = 0.10990161\n",
      "Iteration 15, loss = 0.10349926\n",
      "Iteration 16, loss = 0.09757080\n",
      "Iteration 17, loss = 0.09252204\n",
      "Iteration 18, loss = 0.08717028\n",
      "Iteration 19, loss = 0.08252069\n",
      "Iteration 20, loss = 0.07815521\n",
      "Iteration 21, loss = 0.07471614\n",
      "Iteration 22, loss = 0.07095677\n",
      "Iteration 23, loss = 0.06712141\n",
      "Iteration 24, loss = 0.06429298\n",
      "Iteration 25, loss = 0.06124444\n",
      "Iteration 26, loss = 0.05856482\n",
      "Iteration 27, loss = 0.05619892\n",
      "Iteration 28, loss = 0.05371627\n",
      "Iteration 29, loss = 0.05122041\n",
      "Iteration 30, loss = 0.04930959\n",
      "Iteration 31, loss = 0.04667723\n",
      "Iteration 32, loss = 0.04519330\n",
      "Iteration 33, loss = 0.04317990\n",
      "Iteration 34, loss = 0.04182409\n",
      "Iteration 35, loss = 0.04011045\n",
      "Iteration 36, loss = 0.03858317\n",
      "Iteration 37, loss = 0.03726566\n",
      "Iteration 38, loss = 0.03588337\n",
      "Iteration 39, loss = 0.03439324\n",
      "Iteration 40, loss = 0.03323385\n",
      "Iteration 41, loss = 0.03184386\n",
      "Iteration 42, loss = 0.03099104\n",
      "Iteration 43, loss = 0.02996138\n",
      "Iteration 44, loss = 0.02910606\n",
      "Iteration 45, loss = 0.02796050\n",
      "Iteration 46, loss = 0.02728007\n",
      "Iteration 47, loss = 0.02625853\n",
      "Iteration 48, loss = 0.02544649\n",
      "Iteration 49, loss = 0.02495542\n",
      "Iteration 50, loss = 0.02418416\n",
      "Iteration 51, loss = 0.02348457\n",
      "Iteration 52, loss = 0.02293829\n",
      "Iteration 53, loss = 0.02241621\n",
      "Iteration 54, loss = 0.02186134\n",
      "Iteration 55, loss = 0.02127894\n",
      "Iteration 56, loss = 0.02073382\n",
      "Iteration 57, loss = 0.02042549\n",
      "Iteration 58, loss = 0.01991328\n",
      "Iteration 59, loss = 0.01961157\n",
      "Iteration 60, loss = 0.01917837\n",
      "Iteration 61, loss = 0.01879657\n",
      "Iteration 62, loss = 0.01851464\n",
      "Iteration 63, loss = 0.01819249\n",
      "Iteration 64, loss = 0.01781087\n",
      "Iteration 65, loss = 0.01758386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.01733441\n",
      "Iteration 67, loss = 0.01705114\n",
      "Iteration 68, loss = 0.01682720\n",
      "Iteration 69, loss = 0.01662864\n",
      "Iteration 70, loss = 0.01638417\n",
      "Iteration 71, loss = 0.01618831\n",
      "Iteration 72, loss = 0.01596824\n",
      "Iteration 73, loss = 0.01579813\n",
      "Iteration 74, loss = 0.01568774\n",
      "Iteration 75, loss = 0.01543554\n",
      "Iteration 76, loss = 0.01527961\n",
      "Iteration 77, loss = 0.01514786\n",
      "Iteration 78, loss = 0.01503131\n",
      "Iteration 79, loss = 0.01490040\n",
      "Iteration 80, loss = 0.01474672\n",
      "Iteration 81, loss = 0.01464853\n",
      "Iteration 82, loss = 0.01453787\n",
      "Iteration 83, loss = 0.01440209\n",
      "Iteration 84, loss = 0.01432723\n",
      "Iteration 85, loss = 0.01418460\n",
      "Iteration 86, loss = 0.01408962\n",
      "Iteration 87, loss = 0.01397330\n",
      "Iteration 88, loss = 0.01390409\n",
      "Iteration 89, loss = 0.01380785\n",
      "Iteration 90, loss = 0.01374179\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82902433\n",
      "Iteration 2, loss = 0.47811152\n",
      "Iteration 3, loss = 0.33571414\n",
      "Iteration 4, loss = 0.27630774\n",
      "Iteration 5, loss = 0.23817603\n",
      "Iteration 6, loss = 0.21086037\n",
      "Iteration 7, loss = 0.18894394\n",
      "Iteration 8, loss = 0.17199876\n",
      "Iteration 9, loss = 0.15693992\n",
      "Iteration 10, loss = 0.14517897\n",
      "Iteration 11, loss = 0.13531065\n",
      "Iteration 12, loss = 0.12602354\n",
      "Iteration 13, loss = 0.11771431\n",
      "Iteration 14, loss = 0.11071239\n",
      "Iteration 15, loss = 0.10336068\n",
      "Iteration 16, loss = 0.09723933\n",
      "Iteration 17, loss = 0.09169442\n",
      "Iteration 18, loss = 0.08672395\n",
      "Iteration 19, loss = 0.08147261\n",
      "Iteration 20, loss = 0.07755790\n",
      "Iteration 21, loss = 0.07346314\n",
      "Iteration 22, loss = 0.06975732\n",
      "Iteration 23, loss = 0.06622236\n",
      "Iteration 24, loss = 0.06342035\n",
      "Iteration 25, loss = 0.06009819\n",
      "Iteration 26, loss = 0.05761903\n",
      "Iteration 27, loss = 0.05530611\n",
      "Iteration 28, loss = 0.05291687\n",
      "Iteration 29, loss = 0.05044985\n",
      "Iteration 30, loss = 0.04857772\n",
      "Iteration 31, loss = 0.04605729\n",
      "Iteration 32, loss = 0.04458782\n",
      "Iteration 33, loss = 0.04253133\n",
      "Iteration 34, loss = 0.04090847\n",
      "Iteration 35, loss = 0.03943824\n",
      "Iteration 36, loss = 0.03786955\n",
      "Iteration 37, loss = 0.03655720\n",
      "Iteration 38, loss = 0.03519334\n",
      "Iteration 39, loss = 0.03365141\n",
      "Iteration 40, loss = 0.03279822\n",
      "Iteration 41, loss = 0.03149826\n",
      "Iteration 42, loss = 0.03067758\n",
      "Iteration 43, loss = 0.02934023\n",
      "Iteration 44, loss = 0.02862665\n",
      "Iteration 45, loss = 0.02771650\n",
      "Iteration 46, loss = 0.02665425\n",
      "Iteration 47, loss = 0.02602420\n",
      "Iteration 48, loss = 0.02533104\n",
      "Iteration 49, loss = 0.02441526\n",
      "Iteration 50, loss = 0.02402755\n",
      "Iteration 51, loss = 0.02329349\n",
      "Iteration 52, loss = 0.02260717\n",
      "Iteration 53, loss = 0.02215651\n",
      "Iteration 54, loss = 0.02163405\n",
      "Iteration 55, loss = 0.02089252\n",
      "Iteration 56, loss = 0.02081291\n",
      "Iteration 57, loss = 0.02016964\n",
      "Iteration 58, loss = 0.01977033\n",
      "Iteration 59, loss = 0.01921527\n",
      "Iteration 60, loss = 0.01898645\n",
      "Iteration 61, loss = 0.01853317\n",
      "Iteration 62, loss = 0.01824031\n",
      "Iteration 63, loss = 0.01811182\n",
      "Iteration 64, loss = 0.01772458\n",
      "Iteration 65, loss = 0.01742555\n",
      "Iteration 66, loss = 0.01706534\n",
      "Iteration 67, loss = 0.01675678\n",
      "Iteration 68, loss = 0.01655313\n",
      "Iteration 69, loss = 0.01631482\n",
      "Iteration 70, loss = 0.01611304\n",
      "Iteration 71, loss = 0.01593352\n",
      "Iteration 72, loss = 0.01562591\n",
      "Iteration 73, loss = 0.01555817\n",
      "Iteration 74, loss = 0.01536191\n",
      "Iteration 75, loss = 0.01513819\n",
      "Iteration 76, loss = 0.01499134\n",
      "Iteration 77, loss = 0.01488803\n",
      "Iteration 78, loss = 0.01466022\n",
      "Iteration 79, loss = 0.01457066\n",
      "Iteration 80, loss = 0.01441667\n",
      "Iteration 81, loss = 0.01426539\n",
      "Iteration 82, loss = 0.01416156\n",
      "Iteration 83, loss = 0.01406380\n",
      "Iteration 84, loss = 0.01392955\n",
      "Iteration 85, loss = 0.01387211\n",
      "Iteration 86, loss = 0.01373774\n",
      "Iteration 87, loss = 0.01364885\n",
      "Iteration 88, loss = 0.01355636\n",
      "Iteration 89, loss = 0.01345743\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82863118\n",
      "Iteration 2, loss = 0.48251140\n",
      "Iteration 3, loss = 0.33746019\n",
      "Iteration 4, loss = 0.27927594\n",
      "Iteration 5, loss = 0.24087223\n",
      "Iteration 6, loss = 0.21218045\n",
      "Iteration 7, loss = 0.18944101\n",
      "Iteration 8, loss = 0.17205474\n",
      "Iteration 9, loss = 0.15781236\n",
      "Iteration 10, loss = 0.14539119\n",
      "Iteration 11, loss = 0.13513249\n",
      "Iteration 12, loss = 0.12560033\n",
      "Iteration 13, loss = 0.11806517\n",
      "Iteration 14, loss = 0.11064029\n",
      "Iteration 15, loss = 0.10421976\n",
      "Iteration 16, loss = 0.09864891\n",
      "Iteration 17, loss = 0.09281821\n",
      "Iteration 18, loss = 0.08791210\n",
      "Iteration 19, loss = 0.08357839\n",
      "Iteration 20, loss = 0.07948499\n",
      "Iteration 21, loss = 0.07487683\n",
      "Iteration 22, loss = 0.07119086\n",
      "Iteration 23, loss = 0.06781062\n",
      "Iteration 24, loss = 0.06481856\n",
      "Iteration 25, loss = 0.06177524\n",
      "Iteration 26, loss = 0.05912053\n",
      "Iteration 27, loss = 0.05612705\n",
      "Iteration 28, loss = 0.05361136\n",
      "Iteration 29, loss = 0.05134807\n",
      "Iteration 30, loss = 0.04890348\n",
      "Iteration 31, loss = 0.04744199\n",
      "Iteration 32, loss = 0.04519613\n",
      "Iteration 33, loss = 0.04335775\n",
      "Iteration 34, loss = 0.04155653\n",
      "Iteration 35, loss = 0.03989421\n",
      "Iteration 36, loss = 0.03838576\n",
      "Iteration 37, loss = 0.03684199\n",
      "Iteration 38, loss = 0.03543612\n",
      "Iteration 39, loss = 0.03459868\n",
      "Iteration 40, loss = 0.03310689\n",
      "Iteration 41, loss = 0.03169332\n",
      "Iteration 42, loss = 0.03057456\n",
      "Iteration 43, loss = 0.02979402\n",
      "Iteration 44, loss = 0.02856423\n",
      "Iteration 45, loss = 0.02788464\n",
      "Iteration 46, loss = 0.02705422\n",
      "Iteration 47, loss = 0.02612424\n",
      "Iteration 48, loss = 0.02536528\n",
      "Iteration 49, loss = 0.02443771\n",
      "Iteration 50, loss = 0.02393752\n",
      "Iteration 51, loss = 0.02311308\n",
      "Iteration 52, loss = 0.02252274\n",
      "Iteration 53, loss = 0.02198863\n",
      "Iteration 54, loss = 0.02143445\n",
      "Iteration 55, loss = 0.02107373\n",
      "Iteration 56, loss = 0.02050926\n",
      "Iteration 57, loss = 0.02012638\n",
      "Iteration 58, loss = 0.01964899\n",
      "Iteration 59, loss = 0.01928510\n",
      "Iteration 60, loss = 0.01886273\n",
      "Iteration 61, loss = 0.01846270\n",
      "Iteration 62, loss = 0.01821040\n",
      "Iteration 63, loss = 0.01784784\n",
      "Iteration 64, loss = 0.01754373\n",
      "Iteration 65, loss = 0.01728852\n",
      "Iteration 66, loss = 0.01699149\n",
      "Iteration 67, loss = 0.01679970\n",
      "Iteration 68, loss = 0.01653694\n",
      "Iteration 69, loss = 0.01626509\n",
      "Iteration 70, loss = 0.01613900\n",
      "Iteration 71, loss = 0.01590898\n",
      "Iteration 72, loss = 0.01574313\n",
      "Iteration 73, loss = 0.01555813\n",
      "Iteration 74, loss = 0.01539053\n",
      "Iteration 75, loss = 0.01525878\n",
      "Iteration 76, loss = 0.01506783\n",
      "Iteration 77, loss = 0.01491482\n",
      "Iteration 78, loss = 0.01481284\n",
      "Iteration 79, loss = 0.01464865\n",
      "Iteration 80, loss = 0.01454193\n",
      "Iteration 81, loss = 0.01440169\n",
      "Iteration 82, loss = 0.01425623\n",
      "Iteration 83, loss = 0.01420641\n",
      "Iteration 84, loss = 0.01407852\n",
      "Iteration 85, loss = 0.01397601\n",
      "Iteration 86, loss = 0.01384753\n",
      "Iteration 87, loss = 0.01374833\n",
      "Iteration 88, loss = 0.01369438\n",
      "Iteration 89, loss = 0.01359366\n",
      "Iteration 90, loss = 0.01354159\n",
      "Iteration 91, loss = 0.01342828\n",
      "Iteration 92, loss = 0.01335475\n",
      "Iteration 93, loss = 0.01329651\n",
      "Iteration 94, loss = 0.01323957\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82768422\n",
      "Iteration 2, loss = 0.47819467\n",
      "Iteration 3, loss = 0.33363461\n",
      "Iteration 4, loss = 0.27648574\n",
      "Iteration 5, loss = 0.23753344\n",
      "Iteration 6, loss = 0.20919815\n",
      "Iteration 7, loss = 0.19347291\n",
      "Iteration 8, loss = 0.17084255\n",
      "Iteration 9, loss = 0.15651080\n",
      "Iteration 10, loss = 0.14436446\n",
      "Iteration 11, loss = 0.13425668\n",
      "Iteration 12, loss = 0.20150298\n",
      "Iteration 13, loss = 0.13129792\n",
      "Iteration 14, loss = 0.11781924\n",
      "Iteration 15, loss = 0.10832192\n",
      "Iteration 16, loss = 0.10074824\n",
      "Iteration 17, loss = 0.09517113\n",
      "Iteration 18, loss = 0.09006634\n",
      "Iteration 19, loss = 0.08495415\n",
      "Iteration 20, loss = 0.08035003\n",
      "Iteration 21, loss = 0.07621564\n",
      "Iteration 22, loss = 0.07237244\n",
      "Iteration 23, loss = 0.07067736\n",
      "Iteration 24, loss = 0.06632497\n",
      "Iteration 25, loss = 0.06301622\n",
      "Iteration 26, loss = 0.06078032\n",
      "Iteration 27, loss = 0.05821433\n",
      "Iteration 28, loss = 0.05665384\n",
      "Iteration 29, loss = 0.05357395\n",
      "Iteration 30, loss = 0.05176534\n",
      "Iteration 31, loss = 0.04964909\n",
      "Iteration 32, loss = 0.04762946\n",
      "Iteration 33, loss = 0.04636412\n",
      "Iteration 34, loss = 0.04443750\n",
      "Iteration 35, loss = 0.04354684\n",
      "Iteration 36, loss = 0.04178634\n",
      "Iteration 37, loss = 0.04021658\n",
      "Iteration 38, loss = 0.03876075\n",
      "Iteration 39, loss = 0.03737403\n",
      "Iteration 40, loss = 0.03592809\n",
      "Iteration 41, loss = 0.03510896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.03410158\n",
      "Iteration 43, loss = 0.03297584\n",
      "Iteration 44, loss = 0.03219157\n",
      "Iteration 45, loss = 0.03108494\n",
      "Iteration 46, loss = 0.03014179\n",
      "Iteration 47, loss = 0.02947013\n",
      "Iteration 48, loss = 0.02891317\n",
      "Iteration 49, loss = 0.02763440\n",
      "Iteration 50, loss = 0.02710571\n",
      "Iteration 51, loss = 0.02636165\n",
      "Iteration 52, loss = 0.02569707\n",
      "Iteration 53, loss = 0.02507220\n",
      "Iteration 54, loss = 0.02463479\n",
      "Iteration 55, loss = 0.02406059\n",
      "Iteration 56, loss = 0.02355841\n",
      "Iteration 57, loss = 0.02305389\n",
      "Iteration 58, loss = 0.02255980\n",
      "Iteration 59, loss = 0.02217281\n",
      "Iteration 60, loss = 0.02169957\n",
      "Iteration 61, loss = 0.02132677\n",
      "Iteration 62, loss = 0.02086967\n",
      "Iteration 63, loss = 0.02210017\n",
      "Iteration 64, loss = 0.02055303\n",
      "Iteration 65, loss = 0.01993001\n",
      "Iteration 66, loss = 0.02155773\n",
      "Iteration 67, loss = 0.01958142\n",
      "Iteration 68, loss = 0.01912628\n",
      "Iteration 69, loss = 0.01874220\n",
      "Iteration 70, loss = 0.02209232\n",
      "Iteration 71, loss = 0.01878600\n",
      "Iteration 72, loss = 0.01815398\n",
      "Iteration 73, loss = 0.01784779\n",
      "Iteration 74, loss = 0.01759128\n",
      "Iteration 75, loss = 0.01741722\n",
      "Iteration 76, loss = 0.01718689\n",
      "Iteration 77, loss = 0.01833850\n",
      "Iteration 78, loss = 0.01848837\n",
      "Iteration 79, loss = 0.01704513\n",
      "Iteration 80, loss = 0.01664960\n",
      "Iteration 81, loss = 0.01635085\n",
      "Iteration 82, loss = 0.01629024\n",
      "Iteration 83, loss = 0.01662674\n",
      "Iteration 84, loss = 0.01599529\n",
      "Iteration 85, loss = 0.01577659\n",
      "Iteration 86, loss = 0.01563994\n",
      "Iteration 87, loss = 0.01555631\n",
      "Iteration 88, loss = 0.01549842\n",
      "Iteration 89, loss = 0.01525191\n",
      "Iteration 90, loss = 0.01514793\n",
      "Iteration 91, loss = 0.01513975\n",
      "Iteration 92, loss = 0.01498129\n",
      "Iteration 93, loss = 0.01490686\n",
      "Iteration 94, loss = 0.01484719\n",
      "Iteration 95, loss = 0.01470776\n",
      "Iteration 96, loss = 0.01468586\n",
      "Iteration 97, loss = 0.01454250\n",
      "Iteration 98, loss = 0.01455428\n",
      "Iteration 99, loss = 0.01447892\n",
      "Iteration 100, loss = 0.01435711\n",
      "Iteration 101, loss = 0.01425129\n",
      "Iteration 102, loss = 0.01422467\n",
      "Iteration 103, loss = 0.05821070\n",
      "Iteration 104, loss = 0.02174409\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82679246\n",
      "Iteration 2, loss = 0.48050634\n",
      "Iteration 3, loss = 0.33454088\n",
      "Iteration 4, loss = 0.27764197\n",
      "Iteration 5, loss = 0.23843227\n",
      "Iteration 6, loss = 0.21039189\n",
      "Iteration 7, loss = 0.19200664\n",
      "Iteration 8, loss = 0.17134494\n",
      "Iteration 9, loss = 0.15726393\n",
      "Iteration 10, loss = 0.14458933\n",
      "Iteration 11, loss = 0.13467519\n",
      "Iteration 12, loss = 0.19706332\n",
      "Iteration 13, loss = 0.12890312\n",
      "Iteration 14, loss = 0.11605798\n",
      "Iteration 15, loss = 0.10686069\n",
      "Iteration 16, loss = 0.09993183\n",
      "Iteration 17, loss = 0.09402183\n",
      "Iteration 18, loss = 0.08920765\n",
      "Iteration 19, loss = 0.08435170\n",
      "Iteration 20, loss = 0.07972743\n",
      "Iteration 21, loss = 0.07497902\n",
      "Iteration 22, loss = 0.07136632\n",
      "Iteration 23, loss = 0.06864139\n",
      "Iteration 24, loss = 0.06482282\n",
      "Iteration 25, loss = 0.06163142\n",
      "Iteration 26, loss = 0.05898295\n",
      "Iteration 27, loss = 0.05625436\n",
      "Iteration 28, loss = 0.05670682\n",
      "Iteration 29, loss = 0.05172878\n",
      "Iteration 30, loss = 0.04946386\n",
      "Iteration 31, loss = 0.04798384\n",
      "Iteration 32, loss = 0.04545124\n",
      "Iteration 33, loss = 0.04413220\n",
      "Iteration 34, loss = 0.04215013\n",
      "Iteration 35, loss = 0.05456354\n",
      "Iteration 36, loss = 0.04210660\n",
      "Iteration 37, loss = 0.03920725\n",
      "Iteration 38, loss = 0.03715380\n",
      "Iteration 39, loss = 0.03555007\n",
      "Iteration 40, loss = 0.03416891\n",
      "Iteration 41, loss = 0.03299570\n",
      "Iteration 42, loss = 0.03225656\n",
      "Iteration 43, loss = 0.03099932\n",
      "Iteration 44, loss = 0.03006842\n",
      "Iteration 45, loss = 0.02910205\n",
      "Iteration 46, loss = 0.02828269\n",
      "Iteration 47, loss = 0.02742767\n",
      "Iteration 48, loss = 0.02682146\n",
      "Iteration 49, loss = 0.02955025\n",
      "Iteration 50, loss = 0.02594766\n",
      "Iteration 51, loss = 0.02485422\n",
      "Iteration 52, loss = 0.02398707\n",
      "Iteration 53, loss = 0.02342076\n",
      "Iteration 54, loss = 0.02321365\n",
      "Iteration 55, loss = 0.02241068\n",
      "Iteration 56, loss = 0.02192964\n",
      "Iteration 57, loss = 0.02138240\n",
      "Iteration 58, loss = 0.02100457\n",
      "Iteration 59, loss = 0.02062919\n",
      "Iteration 60, loss = 0.02026592\n",
      "Iteration 61, loss = 0.01979694\n",
      "Iteration 62, loss = 0.01942075\n",
      "Iteration 63, loss = 0.02865346\n",
      "Iteration 64, loss = 0.02019517\n",
      "Iteration 65, loss = 0.01916417\n",
      "Iteration 66, loss = 0.01856861\n",
      "Iteration 67, loss = 0.01816925\n",
      "Iteration 68, loss = 0.01784956\n",
      "Iteration 69, loss = 0.01753976\n",
      "Iteration 70, loss = 0.01789806\n",
      "Iteration 71, loss = 0.01711309\n",
      "Iteration 72, loss = 0.01733999\n",
      "Iteration 73, loss = 0.01677345\n",
      "Iteration 74, loss = 0.01656767\n",
      "Iteration 75, loss = 0.01624327\n",
      "Iteration 76, loss = 0.01608944\n",
      "Iteration 77, loss = 0.01590195\n",
      "Iteration 78, loss = 0.01784875\n",
      "Iteration 79, loss = 0.01599075\n",
      "Iteration 80, loss = 0.01561377\n",
      "Iteration 81, loss = 0.01543801\n",
      "Iteration 82, loss = 0.01528456\n",
      "Iteration 83, loss = 0.01515650\n",
      "Iteration 84, loss = 0.01498593\n",
      "Iteration 85, loss = 0.01487052\n",
      "Iteration 86, loss = 0.01474317\n",
      "Iteration 87, loss = 0.01468438\n",
      "Iteration 88, loss = 0.02028264\n",
      "Iteration 89, loss = 0.01541651\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82711764\n",
      "Iteration 2, loss = 0.48128338\n",
      "Iteration 3, loss = 0.34491779\n",
      "Iteration 4, loss = 0.27973558\n",
      "Iteration 5, loss = 0.24154931\n",
      "Iteration 6, loss = 0.21845050\n",
      "Iteration 7, loss = 0.19273156\n",
      "Iteration 8, loss = 0.19531488\n",
      "Iteration 9, loss = 0.16246593\n",
      "Iteration 10, loss = 0.14938547\n",
      "Iteration 11, loss = 0.13830802\n",
      "Iteration 12, loss = 0.12855417\n",
      "Iteration 13, loss = 0.12114972\n",
      "Iteration 14, loss = 0.11253249\n",
      "Iteration 15, loss = 0.10616848\n",
      "Iteration 16, loss = 0.09958060\n",
      "Iteration 17, loss = 0.09571320\n",
      "Iteration 18, loss = 0.08906759\n",
      "Iteration 19, loss = 0.08427011\n",
      "Iteration 20, loss = 0.08033421\n",
      "Iteration 21, loss = 0.07618809\n",
      "Iteration 22, loss = 0.10227192\n",
      "Iteration 23, loss = 0.08625689\n",
      "Iteration 24, loss = 0.07174978\n",
      "Iteration 25, loss = 0.06649740\n",
      "Iteration 26, loss = 0.08587629\n",
      "Iteration 27, loss = 0.06307178\n",
      "Iteration 28, loss = 0.06534903\n",
      "Iteration 29, loss = 0.05580726\n",
      "Iteration 30, loss = 0.05292338\n",
      "Iteration 31, loss = 0.05052949\n",
      "Iteration 32, loss = 0.04849136\n",
      "Iteration 33, loss = 0.04670986\n",
      "Iteration 34, loss = 0.04449489\n",
      "Iteration 35, loss = 0.04310037\n",
      "Iteration 36, loss = 0.04140443\n",
      "Iteration 37, loss = 0.03994191\n",
      "Iteration 38, loss = 0.03852618\n",
      "Iteration 39, loss = 0.04003470\n",
      "Iteration 40, loss = 0.03670989\n",
      "Iteration 41, loss = 0.03794513\n",
      "Iteration 42, loss = 0.03433685\n",
      "Iteration 43, loss = 0.03317006\n",
      "Iteration 44, loss = 0.03176983\n",
      "Iteration 45, loss = 0.03060568\n",
      "Iteration 46, loss = 0.03004211\n",
      "Iteration 47, loss = 0.03020108\n",
      "Iteration 48, loss = 0.02852080\n",
      "Iteration 49, loss = 0.02748248\n",
      "Iteration 50, loss = 0.02694121\n",
      "Iteration 51, loss = 0.02597521\n",
      "Iteration 52, loss = 0.02558811\n",
      "Iteration 53, loss = 0.02579220\n",
      "Iteration 54, loss = 0.02445419\n",
      "Iteration 55, loss = 0.02477501\n",
      "Iteration 56, loss = 0.02359193\n",
      "Iteration 57, loss = 0.02307856\n",
      "Iteration 58, loss = 0.02240002\n",
      "Iteration 59, loss = 0.02187368\n",
      "Iteration 60, loss = 0.02133876\n",
      "Iteration 61, loss = 0.02102606\n",
      "Iteration 62, loss = 0.02079635\n",
      "Iteration 63, loss = 0.02024831\n",
      "Iteration 64, loss = 0.01997780\n",
      "Iteration 65, loss = 0.01967944\n",
      "Iteration 66, loss = 0.01927397\n",
      "Iteration 67, loss = 0.01900697\n",
      "Iteration 68, loss = 0.01872259\n",
      "Iteration 69, loss = 0.01848309\n",
      "Iteration 70, loss = 0.01825007\n",
      "Iteration 71, loss = 0.01800435\n",
      "Iteration 72, loss = 0.02130860\n",
      "Iteration 73, loss = 0.01816815\n",
      "Iteration 74, loss = 0.01766657\n",
      "Iteration 75, loss = 0.01741874\n",
      "Iteration 76, loss = 0.01698828\n",
      "Iteration 77, loss = 0.01679833\n",
      "Iteration 78, loss = 0.01656100\n",
      "Iteration 79, loss = 0.01639864\n",
      "Iteration 80, loss = 0.01622302\n",
      "Iteration 81, loss = 0.01606835\n",
      "Iteration 82, loss = 0.01589661\n",
      "Iteration 83, loss = 0.01570754\n",
      "Iteration 84, loss = 0.01571998\n",
      "Iteration 85, loss = 0.01554891\n",
      "Iteration 86, loss = 0.01537513\n",
      "Iteration 87, loss = 0.01522845\n",
      "Iteration 88, loss = 0.01510704\n",
      "Iteration 89, loss = 0.01496830\n",
      "Iteration 90, loss = 0.01495057\n",
      "Iteration 91, loss = 0.01471866\n",
      "Iteration 92, loss = 0.01475858\n",
      "Iteration 93, loss = 0.01458457\n",
      "Iteration 94, loss = 0.01449697\n",
      "Iteration 95, loss = 0.01449831\n",
      "Iteration 96, loss = 0.01434516\n",
      "Iteration 97, loss = 0.01421512\n",
      "Iteration 98, loss = 0.01415889\n",
      "Iteration 99, loss = 0.01406746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss = 0.01400639\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82654956\n",
      "Iteration 2, loss = 0.48209011\n",
      "Iteration 3, loss = 0.34315613\n",
      "Iteration 4, loss = 0.28685264\n",
      "Iteration 5, loss = 0.24079742\n",
      "Iteration 6, loss = 0.21211500\n",
      "Iteration 7, loss = 0.19787215\n",
      "Iteration 8, loss = 0.17977520\n",
      "Iteration 9, loss = 0.15850122\n",
      "Iteration 10, loss = 0.16364356\n",
      "Iteration 11, loss = 0.14065655\n",
      "Iteration 12, loss = 0.12867062\n",
      "Iteration 13, loss = 0.12010401\n",
      "Iteration 14, loss = 0.11519620\n",
      "Iteration 15, loss = 0.10439141\n",
      "Iteration 16, loss = 0.10043998\n",
      "Iteration 17, loss = 0.09747824\n",
      "Iteration 18, loss = 0.08848218\n",
      "Iteration 19, loss = 0.09503991\n",
      "Iteration 20, loss = 0.08127551\n",
      "Iteration 21, loss = 0.07613525\n",
      "Iteration 22, loss = 0.07105147\n",
      "Iteration 23, loss = 0.06806593\n",
      "Iteration 24, loss = 0.07907385\n",
      "Iteration 25, loss = 0.06633431\n",
      "Iteration 26, loss = 0.06007680\n",
      "Iteration 27, loss = 0.05677194\n",
      "Iteration 28, loss = 0.05486592\n",
      "Iteration 29, loss = 0.05802059\n",
      "Iteration 30, loss = 0.05053076\n",
      "Iteration 31, loss = 0.04794987\n",
      "Iteration 32, loss = 0.04599443\n",
      "Iteration 33, loss = 0.04372454\n",
      "Iteration 34, loss = 0.04201906\n",
      "Iteration 35, loss = 0.06649782\n",
      "Iteration 36, loss = 0.04507296\n",
      "Iteration 37, loss = 0.04115361\n",
      "Iteration 38, loss = 0.03943173\n",
      "Iteration 39, loss = 0.03720836\n",
      "Iteration 40, loss = 0.03553659\n",
      "Iteration 41, loss = 0.03433784\n",
      "Iteration 42, loss = 0.03259913\n",
      "Iteration 43, loss = 0.03166462\n",
      "Iteration 44, loss = 0.03074678\n",
      "Iteration 45, loss = 0.02962912\n",
      "Iteration 46, loss = 0.02873641\n",
      "Iteration 47, loss = 0.02806582\n",
      "Iteration 48, loss = 0.02807014\n",
      "Iteration 49, loss = 0.02658576\n",
      "Iteration 50, loss = 0.02577269\n",
      "Iteration 51, loss = 0.02517735\n",
      "Iteration 52, loss = 0.02453301\n",
      "Iteration 53, loss = 0.02382327\n",
      "Iteration 54, loss = 0.02450580\n",
      "Iteration 55, loss = 0.02305786\n",
      "Iteration 56, loss = 0.02245081\n",
      "Iteration 57, loss = 0.02192091\n",
      "Iteration 58, loss = 0.02152233\n",
      "Iteration 59, loss = 0.02115605\n",
      "Iteration 60, loss = 0.02073083\n",
      "Iteration 61, loss = 0.02028942\n",
      "Iteration 62, loss = 0.01990303\n",
      "Iteration 63, loss = 0.01959600\n",
      "Iteration 64, loss = 0.01930241\n",
      "Iteration 65, loss = 0.01909338\n",
      "Iteration 66, loss = 0.01873847\n",
      "Iteration 67, loss = 0.01847626\n",
      "Iteration 68, loss = 0.01814017\n",
      "Iteration 69, loss = 0.01793568\n",
      "Iteration 70, loss = 0.01784621\n",
      "Iteration 71, loss = 0.01747467\n",
      "Iteration 72, loss = 0.01717754\n",
      "Iteration 73, loss = 0.01702580\n",
      "Iteration 74, loss = 0.01681016\n",
      "Iteration 75, loss = 0.01664046\n",
      "Iteration 76, loss = 0.01643916\n",
      "Iteration 77, loss = 0.01639655\n",
      "Iteration 78, loss = 0.01611895\n",
      "Iteration 79, loss = 0.01602235\n",
      "Iteration 80, loss = 0.01578098\n",
      "Iteration 81, loss = 0.01561956\n",
      "Iteration 82, loss = 0.01607617\n",
      "Iteration 83, loss = 0.01541921\n",
      "Iteration 84, loss = 0.01524244\n",
      "Iteration 85, loss = 0.01505520\n",
      "Iteration 86, loss = 0.01497501\n",
      "Iteration 87, loss = 0.01481725\n",
      "Iteration 88, loss = 0.01469273\n",
      "Iteration 89, loss = 0.01459208\n",
      "Iteration 90, loss = 0.01450967\n",
      "Iteration 91, loss = 0.01438605\n",
      "Iteration 92, loss = 0.01430004\n",
      "Iteration 93, loss = 0.01421134\n",
      "Iteration 94, loss = 0.01502672\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82866313\n",
      "Iteration 2, loss = 0.48209295\n",
      "Iteration 3, loss = 0.34287949\n",
      "Iteration 4, loss = 0.28522024\n",
      "Iteration 5, loss = 0.24097796\n",
      "Iteration 6, loss = 0.21205314\n",
      "Iteration 7, loss = 0.19782136\n",
      "Iteration 8, loss = 0.17895965\n",
      "Iteration 9, loss = 0.15965973\n",
      "Iteration 10, loss = 0.16536813\n",
      "Iteration 11, loss = 0.13868854\n",
      "Iteration 12, loss = 0.12771461\n",
      "Iteration 13, loss = 0.11965107\n",
      "Iteration 14, loss = 0.11920659\n",
      "Iteration 15, loss = 0.10614784\n",
      "Iteration 16, loss = 0.10125336\n",
      "Iteration 17, loss = 0.09435149\n",
      "Iteration 18, loss = 0.08948710\n",
      "Iteration 19, loss = 0.10466053\n",
      "Iteration 20, loss = 0.08352741\n",
      "Iteration 21, loss = 0.07861729\n",
      "Iteration 22, loss = 0.07394437\n",
      "Iteration 23, loss = 0.07075289\n",
      "Iteration 24, loss = 0.07221017\n",
      "Iteration 25, loss = 0.06515334\n",
      "Iteration 26, loss = 0.06157709\n",
      "Iteration 27, loss = 0.05895915\n",
      "Iteration 28, loss = 0.05672454\n",
      "Iteration 29, loss = 0.06014876\n",
      "Iteration 30, loss = 0.05279753\n",
      "Iteration 31, loss = 0.05032198\n",
      "Iteration 32, loss = 0.04800109\n",
      "Iteration 33, loss = 0.04603414\n",
      "Iteration 34, loss = 0.04413648\n",
      "Iteration 35, loss = 0.06376699\n",
      "Iteration 36, loss = 0.04421126\n",
      "Iteration 37, loss = 0.04134152\n",
      "Iteration 38, loss = 0.03968230\n",
      "Iteration 39, loss = 0.03755796\n",
      "Iteration 40, loss = 0.04311866\n",
      "Iteration 41, loss = 0.03642108\n",
      "Iteration 42, loss = 0.03485621\n",
      "Iteration 43, loss = 0.03309919\n",
      "Iteration 44, loss = 0.03193499\n",
      "Iteration 45, loss = 0.03068504\n",
      "Iteration 46, loss = 0.02987945\n",
      "Iteration 47, loss = 0.02918356\n",
      "Iteration 48, loss = 0.02805658\n",
      "Iteration 49, loss = 0.02716761\n",
      "Iteration 50, loss = 0.02642946\n",
      "Iteration 51, loss = 0.02579460\n",
      "Iteration 52, loss = 0.02504915\n",
      "Iteration 53, loss = 0.02444623\n",
      "Iteration 54, loss = 0.02963153\n",
      "Iteration 55, loss = 0.02431920\n",
      "Iteration 56, loss = 0.02350886\n",
      "Iteration 57, loss = 0.02259847\n",
      "Iteration 58, loss = 0.02213339\n",
      "Iteration 59, loss = 0.02156445\n",
      "Iteration 60, loss = 0.02102268\n",
      "Iteration 61, loss = 0.02064834\n",
      "Iteration 62, loss = 0.02034787\n",
      "Iteration 63, loss = 0.01991596\n",
      "Iteration 64, loss = 0.01966172\n",
      "Iteration 65, loss = 0.01925602\n",
      "Iteration 66, loss = 0.01895228\n",
      "Iteration 67, loss = 0.01864000\n",
      "Iteration 68, loss = 0.01834082\n",
      "Iteration 69, loss = 0.01809277\n",
      "Iteration 70, loss = 0.02112412\n",
      "Iteration 71, loss = 0.01829386\n",
      "Iteration 72, loss = 0.01840174\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83478820\n",
      "Iteration 2, loss = 0.45344841\n",
      "Iteration 3, loss = 0.41772120\n",
      "Iteration 4, loss = 0.39787227\n",
      "Iteration 5, loss = 0.40266499\n",
      "Iteration 6, loss = 0.41167426\n",
      "Iteration 7, loss = 0.41256492\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75534417\n",
      "Iteration 2, loss = 0.42358376\n",
      "Iteration 3, loss = 0.41540378\n",
      "Iteration 4, loss = 0.40970808\n",
      "Iteration 5, loss = 0.40556723\n",
      "Iteration 6, loss = 0.40735765\n",
      "Iteration 7, loss = 0.40777963\n",
      "Iteration 8, loss = 0.41662471\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17401340\n",
      "Iteration 2, loss = 0.57001514\n",
      "Iteration 3, loss = 0.51647433\n",
      "Iteration 4, loss = 0.48979402\n",
      "Iteration 5, loss = 0.45624738\n",
      "Iteration 6, loss = 0.44086549\n",
      "Iteration 7, loss = 0.43507273\n",
      "Iteration 8, loss = 0.43194583\n",
      "Iteration 9, loss = 0.43664004\n",
      "Iteration 10, loss = 0.42945583\n",
      "Iteration 11, loss = 0.42990603\n",
      "Iteration 12, loss = 0.42582830\n",
      "Iteration 13, loss = 0.43990448\n",
      "Iteration 14, loss = 0.43574019\n",
      "Iteration 15, loss = 0.43485289\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85833988\n",
      "Iteration 2, loss = 0.47208769\n",
      "Iteration 3, loss = 0.44985728\n",
      "Iteration 4, loss = 0.44165153\n",
      "Iteration 5, loss = 0.42911633\n",
      "Iteration 6, loss = 0.43182006\n",
      "Iteration 7, loss = 0.42906364\n",
      "Iteration 8, loss = 0.42557645\n",
      "Iteration 9, loss = 0.44388509\n",
      "Iteration 10, loss = 0.43191410\n",
      "Iteration 11, loss = 0.43274612\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88156252\n",
      "Iteration 2, loss = 0.45818233\n",
      "Iteration 3, loss = 0.43623087\n",
      "Iteration 4, loss = 0.42651414\n",
      "Iteration 5, loss = 0.42434722\n",
      "Iteration 6, loss = 0.41561153\n",
      "Iteration 7, loss = 0.41876890\n",
      "Iteration 8, loss = 0.42735529\n",
      "Iteration 9, loss = 0.43621765\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97199902\n",
      "Iteration 2, loss = 0.50667776\n",
      "Iteration 3, loss = 0.45150744\n",
      "Iteration 4, loss = 0.41657643\n",
      "Iteration 5, loss = 0.41460011\n",
      "Iteration 6, loss = 0.40076352\n",
      "Iteration 7, loss = 0.47436598\n",
      "Iteration 8, loss = 0.41774440\n",
      "Iteration 9, loss = 0.41774004\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82067126\n",
      "Iteration 2, loss = 0.41632608\n",
      "Iteration 3, loss = 0.38320959\n",
      "Iteration 4, loss = 0.39176843\n",
      "Iteration 5, loss = 0.39026051\n",
      "Iteration 6, loss = 0.37707858\n",
      "Iteration 7, loss = 0.40255947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.37964147\n",
      "Iteration 9, loss = 0.41345812\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89057542\n",
      "Iteration 2, loss = 0.43399595\n",
      "Iteration 3, loss = 0.42255788\n",
      "Iteration 4, loss = 0.39052530\n",
      "Iteration 5, loss = 0.39617021\n",
      "Iteration 6, loss = 0.48914009\n",
      "Iteration 7, loss = 0.41351416\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79932357\n",
      "Iteration 2, loss = 0.43807295\n",
      "Iteration 3, loss = 0.42407670\n",
      "Iteration 4, loss = 0.42817426\n",
      "Iteration 5, loss = 0.42445706\n",
      "Iteration 6, loss = 0.41727663\n",
      "Iteration 7, loss = 0.43503732\n",
      "Iteration 8, loss = 0.44048713\n",
      "Iteration 9, loss = 0.45699643\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81868908\n",
      "Iteration 2, loss = 0.45049843\n",
      "Iteration 3, loss = 0.43347403\n",
      "Iteration 4, loss = 0.45905147\n",
      "Iteration 5, loss = 0.42731622\n",
      "Iteration 6, loss = 0.42510966\n",
      "Iteration 7, loss = 0.42361811\n",
      "Iteration 8, loss = 0.45904155\n",
      "Iteration 9, loss = 0.42818545\n",
      "Iteration 10, loss = 0.46457017\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82940564\n",
      "Iteration 2, loss = 0.48237793\n",
      "Iteration 3, loss = 0.33687559\n",
      "Iteration 4, loss = 0.27841100\n",
      "Iteration 5, loss = 0.23966963\n",
      "Iteration 6, loss = 0.21148928\n",
      "Iteration 7, loss = 0.18920070\n",
      "Iteration 8, loss = 0.17147598\n",
      "Iteration 9, loss = 0.15659738\n",
      "Iteration 10, loss = 0.14414448\n",
      "Iteration 11, loss = 0.13371198\n",
      "Iteration 12, loss = 0.12380111\n",
      "Iteration 13, loss = 0.11570395\n",
      "Iteration 14, loss = 0.10813666\n",
      "Iteration 15, loss = 0.10151470\n",
      "Iteration 16, loss = 0.09580231\n",
      "Iteration 17, loss = 0.08999539\n",
      "Iteration 18, loss = 0.08469456\n",
      "Iteration 19, loss = 0.08065376\n",
      "Iteration 20, loss = 0.07590066\n",
      "Iteration 21, loss = 0.07176349\n",
      "Iteration 22, loss = 0.06864750\n",
      "Iteration 23, loss = 0.06503300\n",
      "Iteration 24, loss = 0.06168482\n",
      "Iteration 25, loss = 0.05881051\n",
      "Iteration 26, loss = 0.05582438\n",
      "Iteration 27, loss = 0.05313349\n",
      "Iteration 28, loss = 0.05044019\n",
      "Iteration 29, loss = 0.04808673\n",
      "Iteration 30, loss = 0.04589397\n",
      "Iteration 31, loss = 0.04366847\n",
      "Iteration 32, loss = 0.04191526\n",
      "Iteration 33, loss = 0.04007709\n",
      "Iteration 34, loss = 0.03847439\n",
      "Iteration 35, loss = 0.03651387\n",
      "Iteration 36, loss = 0.03514363\n",
      "Iteration 37, loss = 0.03349809\n",
      "Iteration 38, loss = 0.03231457\n",
      "Iteration 39, loss = 0.03097264\n",
      "Iteration 40, loss = 0.02971152\n",
      "Iteration 41, loss = 0.02824374\n",
      "Iteration 42, loss = 0.02722012\n",
      "Iteration 43, loss = 0.02616242\n",
      "Iteration 44, loss = 0.02540507\n",
      "Iteration 45, loss = 0.02441570\n",
      "Iteration 46, loss = 0.02321220\n",
      "Iteration 47, loss = 0.02244237\n",
      "Iteration 48, loss = 0.02158958\n",
      "Iteration 49, loss = 0.02105236\n",
      "Iteration 50, loss = 0.02010559\n",
      "Iteration 51, loss = 0.01971930\n",
      "Iteration 52, loss = 0.01884774\n",
      "Iteration 53, loss = 0.01812382\n",
      "Iteration 54, loss = 0.01763443\n",
      "Iteration 55, loss = 0.01700157\n",
      "Iteration 56, loss = 0.01648250\n",
      "Iteration 57, loss = 0.01588731\n",
      "Iteration 58, loss = 0.01543377\n",
      "Iteration 59, loss = 0.01500481\n",
      "Iteration 60, loss = 0.01456131\n",
      "Iteration 61, loss = 0.01423153\n",
      "Iteration 62, loss = 0.01366484\n",
      "Iteration 63, loss = 0.01333344\n",
      "Iteration 64, loss = 0.01300917\n",
      "Iteration 65, loss = 0.01262942\n",
      "Iteration 66, loss = 0.01229100\n",
      "Iteration 67, loss = 0.01205428\n",
      "Iteration 68, loss = 0.01168025\n",
      "Iteration 69, loss = 0.01141326\n",
      "Iteration 70, loss = 0.01120833\n",
      "Iteration 71, loss = 0.01096694\n",
      "Iteration 72, loss = 0.01077249\n",
      "Iteration 73, loss = 0.01050304\n",
      "Iteration 74, loss = 0.01029977\n",
      "Iteration 75, loss = 0.01011293\n",
      "Iteration 76, loss = 0.00993416\n",
      "Iteration 77, loss = 0.00970122\n",
      "Iteration 78, loss = 0.00958054\n",
      "Iteration 79, loss = 0.00940451\n",
      "Iteration 80, loss = 0.00923772\n",
      "Iteration 81, loss = 0.00908351\n",
      "Iteration 82, loss = 0.00897438\n",
      "Iteration 83, loss = 0.00881491\n",
      "Iteration 84, loss = 0.00870503\n",
      "Iteration 85, loss = 0.00854800\n",
      "Iteration 86, loss = 0.00848102\n",
      "Iteration 87, loss = 0.00832887\n",
      "Iteration 88, loss = 0.00819595\n",
      "Iteration 89, loss = 0.00813157\n",
      "Iteration 90, loss = 0.00799131\n",
      "Iteration 91, loss = 0.00790896\n",
      "Iteration 92, loss = 0.00780623\n",
      "Iteration 93, loss = 0.00771420\n",
      "Iteration 94, loss = 0.00759979\n",
      "Iteration 95, loss = 0.00752267\n",
      "Iteration 96, loss = 0.00743718\n",
      "Iteration 97, loss = 0.00736354\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83128610\n",
      "Iteration 2, loss = 0.48111156\n",
      "Iteration 3, loss = 0.33447878\n",
      "Iteration 4, loss = 0.27714992\n",
      "Iteration 5, loss = 0.23832596\n",
      "Iteration 6, loss = 0.21076578\n",
      "Iteration 7, loss = 0.19000742\n",
      "Iteration 8, loss = 0.17208303\n",
      "Iteration 9, loss = 0.15656093\n",
      "Iteration 10, loss = 0.14364423\n",
      "Iteration 11, loss = 0.13283843\n",
      "Iteration 12, loss = 0.12346416\n",
      "Iteration 13, loss = 0.11510555\n",
      "Iteration 14, loss = 0.10736372\n",
      "Iteration 15, loss = 0.10064365\n",
      "Iteration 16, loss = 0.09430887\n",
      "Iteration 17, loss = 0.08829330\n",
      "Iteration 18, loss = 0.08349676\n",
      "Iteration 19, loss = 0.07846458\n",
      "Iteration 20, loss = 0.07407686\n",
      "Iteration 21, loss = 0.06967342\n",
      "Iteration 22, loss = 0.06580495\n",
      "Iteration 23, loss = 0.06273432\n",
      "Iteration 24, loss = 0.05927094\n",
      "Iteration 25, loss = 0.05638132\n",
      "Iteration 26, loss = 0.05372647\n",
      "Iteration 27, loss = 0.05118306\n",
      "Iteration 28, loss = 0.04843340\n",
      "Iteration 29, loss = 0.04658449\n",
      "Iteration 30, loss = 0.04416236\n",
      "Iteration 31, loss = 0.04187007\n",
      "Iteration 32, loss = 0.04020240\n",
      "Iteration 33, loss = 0.03847481\n",
      "Iteration 34, loss = 0.03665987\n",
      "Iteration 35, loss = 0.03472902\n",
      "Iteration 36, loss = 0.03345713\n",
      "Iteration 37, loss = 0.03199816\n",
      "Iteration 38, loss = 0.03069163\n",
      "Iteration 39, loss = 0.02953404\n",
      "Iteration 40, loss = 0.02800186\n",
      "Iteration 41, loss = 0.02691112\n",
      "Iteration 42, loss = 0.02581735\n",
      "Iteration 43, loss = 0.02474665\n",
      "Iteration 44, loss = 0.02388946\n",
      "Iteration 45, loss = 0.02278407\n",
      "Iteration 46, loss = 0.02182164\n",
      "Iteration 47, loss = 0.02088429\n",
      "Iteration 48, loss = 0.02010789\n",
      "Iteration 49, loss = 0.01963653\n",
      "Iteration 50, loss = 0.01888361\n",
      "Iteration 51, loss = 0.01841620\n",
      "Iteration 52, loss = 0.01759013\n",
      "Iteration 53, loss = 0.01713019\n",
      "Iteration 54, loss = 0.01647920\n",
      "Iteration 55, loss = 0.01599422\n",
      "Iteration 56, loss = 0.01542102\n",
      "Iteration 57, loss = 0.01493213\n",
      "Iteration 58, loss = 0.01451000\n",
      "Iteration 59, loss = 0.01410422\n",
      "Iteration 60, loss = 0.01370699\n",
      "Iteration 61, loss = 0.01328533\n",
      "Iteration 62, loss = 0.01303751\n",
      "Iteration 63, loss = 0.01265404\n",
      "Iteration 64, loss = 0.01225602\n",
      "Iteration 65, loss = 0.01199996\n",
      "Iteration 66, loss = 0.01164854\n",
      "Iteration 67, loss = 0.01142919\n",
      "Iteration 68, loss = 0.01107165\n",
      "Iteration 69, loss = 0.01085806\n",
      "Iteration 70, loss = 0.01063987\n",
      "Iteration 71, loss = 0.01040097\n",
      "Iteration 72, loss = 0.01015591\n",
      "Iteration 73, loss = 0.00994153\n",
      "Iteration 74, loss = 0.00971675\n",
      "Iteration 75, loss = 0.00955083\n",
      "Iteration 76, loss = 0.00941311\n",
      "Iteration 77, loss = 0.00921449\n",
      "Iteration 78, loss = 0.00903132\n",
      "Iteration 79, loss = 0.00885699\n",
      "Iteration 80, loss = 0.00876719\n",
      "Iteration 81, loss = 0.00857616\n",
      "Iteration 82, loss = 0.00845688\n",
      "Iteration 83, loss = 0.00831610\n",
      "Iteration 84, loss = 0.00821500\n",
      "Iteration 85, loss = 0.00808995\n",
      "Iteration 86, loss = 0.00797477\n",
      "Iteration 87, loss = 0.00786177\n",
      "Iteration 88, loss = 0.00773390\n",
      "Iteration 89, loss = 0.00761378\n",
      "Iteration 90, loss = 0.00755662\n",
      "Iteration 91, loss = 0.00742125\n",
      "Iteration 92, loss = 0.00736824\n",
      "Iteration 93, loss = 0.00728236\n",
      "Iteration 94, loss = 0.00719420\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82920698\n",
      "Iteration 2, loss = 0.48188875\n",
      "Iteration 3, loss = 0.33866346\n",
      "Iteration 4, loss = 0.27870617\n",
      "Iteration 5, loss = 0.24019029\n",
      "Iteration 6, loss = 0.21192236\n",
      "Iteration 7, loss = 0.18926584\n",
      "Iteration 8, loss = 0.17165549\n",
      "Iteration 9, loss = 0.15739783\n",
      "Iteration 10, loss = 0.14438271\n",
      "Iteration 11, loss = 0.13293414\n",
      "Iteration 12, loss = 0.12356520\n",
      "Iteration 13, loss = 0.11503687\n",
      "Iteration 14, loss = 0.10730043\n",
      "Iteration 15, loss = 0.10079440\n",
      "Iteration 16, loss = 0.09475787\n",
      "Iteration 17, loss = 0.08960596\n",
      "Iteration 18, loss = 0.08415985\n",
      "Iteration 19, loss = 0.07941193\n",
      "Iteration 20, loss = 0.07493976\n",
      "Iteration 21, loss = 0.07140967\n",
      "Iteration 22, loss = 0.06754877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.06361739\n",
      "Iteration 24, loss = 0.06070348\n",
      "Iteration 25, loss = 0.05756549\n",
      "Iteration 26, loss = 0.05479367\n",
      "Iteration 27, loss = 0.05233603\n",
      "Iteration 28, loss = 0.04976664\n",
      "Iteration 29, loss = 0.04718139\n",
      "Iteration 30, loss = 0.04518503\n",
      "Iteration 31, loss = 0.04248305\n",
      "Iteration 32, loss = 0.04090884\n",
      "Iteration 33, loss = 0.03883367\n",
      "Iteration 34, loss = 0.03738632\n",
      "Iteration 35, loss = 0.03561426\n",
      "Iteration 36, loss = 0.03398884\n",
      "Iteration 37, loss = 0.03261691\n",
      "Iteration 38, loss = 0.03115231\n",
      "Iteration 39, loss = 0.02960026\n",
      "Iteration 40, loss = 0.02835090\n",
      "Iteration 41, loss = 0.02692863\n",
      "Iteration 42, loss = 0.02603258\n",
      "Iteration 43, loss = 0.02493094\n",
      "Iteration 44, loss = 0.02402705\n",
      "Iteration 45, loss = 0.02285210\n",
      "Iteration 46, loss = 0.02209281\n",
      "Iteration 47, loss = 0.02104426\n",
      "Iteration 48, loss = 0.02020722\n",
      "Iteration 49, loss = 0.01966229\n",
      "Iteration 50, loss = 0.01884580\n",
      "Iteration 51, loss = 0.01812914\n",
      "Iteration 52, loss = 0.01755977\n",
      "Iteration 53, loss = 0.01698066\n",
      "Iteration 54, loss = 0.01639631\n",
      "Iteration 55, loss = 0.01579366\n",
      "Iteration 56, loss = 0.01521416\n",
      "Iteration 57, loss = 0.01486641\n",
      "Iteration 58, loss = 0.01432969\n",
      "Iteration 59, loss = 0.01399819\n",
      "Iteration 60, loss = 0.01354202\n",
      "Iteration 61, loss = 0.01314091\n",
      "Iteration 62, loss = 0.01282814\n",
      "Iteration 63, loss = 0.01248896\n",
      "Iteration 64, loss = 0.01208440\n",
      "Iteration 65, loss = 0.01183502\n",
      "Iteration 66, loss = 0.01157075\n",
      "Iteration 67, loss = 0.01126782\n",
      "Iteration 68, loss = 0.01102058\n",
      "Iteration 69, loss = 0.01079835\n",
      "Iteration 70, loss = 0.01054370\n",
      "Iteration 71, loss = 0.01032977\n",
      "Iteration 72, loss = 0.01009892\n",
      "Iteration 73, loss = 0.00990918\n",
      "Iteration 74, loss = 0.00976782\n",
      "Iteration 75, loss = 0.00951982\n",
      "Iteration 76, loss = 0.00934554\n",
      "Iteration 77, loss = 0.00919619\n",
      "Iteration 78, loss = 0.00905629\n",
      "Iteration 79, loss = 0.00891332\n",
      "Iteration 80, loss = 0.00874142\n",
      "Iteration 81, loss = 0.00863325\n",
      "Iteration 82, loss = 0.00850571\n",
      "Iteration 83, loss = 0.00835981\n",
      "Iteration 84, loss = 0.00826481\n",
      "Iteration 85, loss = 0.00811304\n",
      "Iteration 86, loss = 0.00800914\n",
      "Iteration 87, loss = 0.00788617\n",
      "Iteration 88, loss = 0.00780172\n",
      "Iteration 89, loss = 0.00769601\n",
      "Iteration 90, loss = 0.00761590\n",
      "Iteration 91, loss = 0.00750062\n",
      "Iteration 92, loss = 0.00742374\n",
      "Iteration 93, loss = 0.00734548\n",
      "Iteration 94, loss = 0.00725409\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82860383\n",
      "Iteration 2, loss = 0.47721121\n",
      "Iteration 3, loss = 0.33460091\n",
      "Iteration 4, loss = 0.27500572\n",
      "Iteration 5, loss = 0.23671377\n",
      "Iteration 6, loss = 0.20924745\n",
      "Iteration 7, loss = 0.18718963\n",
      "Iteration 8, loss = 0.17010849\n",
      "Iteration 9, loss = 0.15491535\n",
      "Iteration 10, loss = 0.14303138\n",
      "Iteration 11, loss = 0.13304413\n",
      "Iteration 12, loss = 0.12364371\n",
      "Iteration 13, loss = 0.11522064\n",
      "Iteration 14, loss = 0.10810847\n",
      "Iteration 15, loss = 0.10065515\n",
      "Iteration 16, loss = 0.09442975\n",
      "Iteration 17, loss = 0.08878898\n",
      "Iteration 18, loss = 0.08371763\n",
      "Iteration 19, loss = 0.07837764\n",
      "Iteration 20, loss = 0.07437134\n",
      "Iteration 21, loss = 0.07017708\n",
      "Iteration 22, loss = 0.06638359\n",
      "Iteration 23, loss = 0.06276114\n",
      "Iteration 24, loss = 0.05987735\n",
      "Iteration 25, loss = 0.05646338\n",
      "Iteration 26, loss = 0.05391134\n",
      "Iteration 27, loss = 0.05151398\n",
      "Iteration 28, loss = 0.04903278\n",
      "Iteration 29, loss = 0.04649375\n",
      "Iteration 30, loss = 0.04455363\n",
      "Iteration 31, loss = 0.04193952\n",
      "Iteration 32, loss = 0.04040099\n",
      "Iteration 33, loss = 0.03827131\n",
      "Iteration 34, loss = 0.03656370\n",
      "Iteration 35, loss = 0.03502046\n",
      "Iteration 36, loss = 0.03340449\n",
      "Iteration 37, loss = 0.03201217\n",
      "Iteration 38, loss = 0.03058676\n",
      "Iteration 39, loss = 0.02897928\n",
      "Iteration 40, loss = 0.02807466\n",
      "Iteration 41, loss = 0.02672006\n",
      "Iteration 42, loss = 0.02582775\n",
      "Iteration 43, loss = 0.02446071\n",
      "Iteration 44, loss = 0.02369637\n",
      "Iteration 45, loss = 0.02273574\n",
      "Iteration 46, loss = 0.02163321\n",
      "Iteration 47, loss = 0.02095316\n",
      "Iteration 48, loss = 0.02021581\n",
      "Iteration 49, loss = 0.01927675\n",
      "Iteration 50, loss = 0.01883095\n",
      "Iteration 51, loss = 0.01806482\n",
      "Iteration 52, loss = 0.01734539\n",
      "Iteration 53, loss = 0.01684358\n",
      "Iteration 54, loss = 0.01628589\n",
      "Iteration 55, loss = 0.01552962\n",
      "Iteration 56, loss = 0.01539487\n",
      "Iteration 57, loss = 0.01471967\n",
      "Iteration 58, loss = 0.01430097\n",
      "Iteration 59, loss = 0.01372229\n",
      "Iteration 60, loss = 0.01346742\n",
      "Iteration 61, loss = 0.01299232\n",
      "Iteration 62, loss = 0.01266677\n",
      "Iteration 63, loss = 0.01248683\n",
      "Iteration 64, loss = 0.01208403\n",
      "Iteration 65, loss = 0.01176677\n",
      "Iteration 66, loss = 0.01138999\n",
      "Iteration 67, loss = 0.01104626\n",
      "Iteration 68, loss = 0.01081529\n",
      "Iteration 69, loss = 0.01056112\n",
      "Iteration 70, loss = 0.01033242\n",
      "Iteration 71, loss = 0.01012329\n",
      "Iteration 72, loss = 0.00982679\n",
      "Iteration 73, loss = 0.00972620\n",
      "Iteration 74, loss = 0.00950763\n",
      "Iteration 75, loss = 0.00927934\n",
      "Iteration 76, loss = 0.00911687\n",
      "Iteration 77, loss = 0.00898885\n",
      "Iteration 78, loss = 0.00877451\n",
      "Iteration 79, loss = 0.00864951\n",
      "Iteration 80, loss = 0.00849101\n",
      "Iteration 81, loss = 0.00833395\n",
      "Iteration 82, loss = 0.00821573\n",
      "Iteration 83, loss = 0.00810195\n",
      "Iteration 84, loss = 0.00796205\n",
      "Iteration 85, loss = 0.00788377\n",
      "Iteration 86, loss = 0.00774843\n",
      "Iteration 87, loss = 0.00764573\n",
      "Iteration 88, loss = 0.00754272\n",
      "Iteration 89, loss = 0.00743740\n",
      "Iteration 90, loss = 0.00732793\n",
      "Iteration 91, loss = 0.00725441\n",
      "Iteration 92, loss = 0.00717174\n",
      "Iteration 93, loss = 0.00710449\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82821203\n",
      "Iteration 2, loss = 0.48161300\n",
      "Iteration 3, loss = 0.33634966\n",
      "Iteration 4, loss = 0.27797888\n",
      "Iteration 5, loss = 0.23940975\n",
      "Iteration 6, loss = 0.21056130\n",
      "Iteration 7, loss = 0.18767941\n",
      "Iteration 8, loss = 0.17015694\n",
      "Iteration 9, loss = 0.15577994\n",
      "Iteration 10, loss = 0.14323352\n",
      "Iteration 11, loss = 0.13284877\n",
      "Iteration 12, loss = 0.12320686\n",
      "Iteration 13, loss = 0.11555716\n",
      "Iteration 14, loss = 0.10803008\n",
      "Iteration 15, loss = 0.10150313\n",
      "Iteration 16, loss = 0.09583622\n",
      "Iteration 17, loss = 0.08990195\n",
      "Iteration 18, loss = 0.08490129\n",
      "Iteration 19, loss = 0.08047336\n",
      "Iteration 20, loss = 0.07628529\n",
      "Iteration 21, loss = 0.07158026\n",
      "Iteration 22, loss = 0.06779962\n",
      "Iteration 23, loss = 0.06431963\n",
      "Iteration 24, loss = 0.06124374\n",
      "Iteration 25, loss = 0.05810834\n",
      "Iteration 26, loss = 0.05537271\n",
      "Iteration 27, loss = 0.05230708\n",
      "Iteration 28, loss = 0.04968603\n",
      "Iteration 29, loss = 0.04735185\n",
      "Iteration 30, loss = 0.04483335\n",
      "Iteration 31, loss = 0.04330892\n",
      "Iteration 32, loss = 0.04098548\n",
      "Iteration 33, loss = 0.03905352\n",
      "Iteration 34, loss = 0.03719753\n",
      "Iteration 35, loss = 0.03546680\n",
      "Iteration 36, loss = 0.03389424\n",
      "Iteration 37, loss = 0.03227330\n",
      "Iteration 38, loss = 0.03081070\n",
      "Iteration 39, loss = 0.02990840\n",
      "Iteration 40, loss = 0.02835632\n",
      "Iteration 41, loss = 0.02689811\n",
      "Iteration 42, loss = 0.02573842\n",
      "Iteration 43, loss = 0.02489039\n",
      "Iteration 44, loss = 0.02360707\n",
      "Iteration 45, loss = 0.02287760\n",
      "Iteration 46, loss = 0.02200414\n",
      "Iteration 47, loss = 0.02104513\n",
      "Iteration 48, loss = 0.02020290\n",
      "Iteration 49, loss = 0.01925861\n",
      "Iteration 50, loss = 0.01870990\n",
      "Iteration 51, loss = 0.01785054\n",
      "Iteration 52, loss = 0.01723785\n",
      "Iteration 53, loss = 0.01666187\n",
      "Iteration 54, loss = 0.01607887\n",
      "Iteration 55, loss = 0.01568073\n",
      "Iteration 56, loss = 0.01507564\n",
      "Iteration 57, loss = 0.01466823\n",
      "Iteration 58, loss = 0.01416841\n",
      "Iteration 59, loss = 0.01377036\n",
      "Iteration 60, loss = 0.01333289\n",
      "Iteration 61, loss = 0.01291808\n",
      "Iteration 62, loss = 0.01263681\n",
      "Iteration 63, loss = 0.01226045\n",
      "Iteration 64, loss = 0.01194313\n",
      "Iteration 65, loss = 0.01166694\n",
      "Iteration 66, loss = 0.01135414\n",
      "Iteration 67, loss = 0.01113683\n",
      "Iteration 68, loss = 0.01085739\n",
      "Iteration 69, loss = 0.01057581\n",
      "Iteration 70, loss = 0.01041411\n",
      "Iteration 71, loss = 0.01017598\n",
      "Iteration 72, loss = 0.00997879\n",
      "Iteration 73, loss = 0.00978285\n",
      "Iteration 74, loss = 0.00960249\n",
      "Iteration 75, loss = 0.00944494\n",
      "Iteration 76, loss = 0.00924200\n",
      "Iteration 77, loss = 0.00906526\n",
      "Iteration 78, loss = 0.00895097\n",
      "Iteration 79, loss = 0.00877652\n",
      "Iteration 80, loss = 0.00865409\n",
      "Iteration 81, loss = 0.00850445\n",
      "Iteration 82, loss = 0.00834700\n",
      "Iteration 83, loss = 0.00826988\n",
      "Iteration 84, loss = 0.00812905\n",
      "Iteration 85, loss = 0.00801847\n",
      "Iteration 86, loss = 0.00787051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87, loss = 0.00776459\n",
      "Iteration 88, loss = 0.00768642\n",
      "Iteration 89, loss = 0.00757277\n",
      "Iteration 90, loss = 0.00750101\n",
      "Iteration 91, loss = 0.00737691\n",
      "Iteration 92, loss = 0.00729235\n",
      "Iteration 93, loss = 0.00721612\n",
      "Iteration 94, loss = 0.00714675\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82726153\n",
      "Iteration 2, loss = 0.47714061\n",
      "Iteration 3, loss = 0.33236299\n",
      "Iteration 4, loss = 0.27499180\n",
      "Iteration 5, loss = 0.23584379\n",
      "Iteration 6, loss = 0.20732856\n",
      "Iteration 7, loss = 0.19137249\n",
      "Iteration 8, loss = 0.16863516\n",
      "Iteration 9, loss = 0.15414170\n",
      "Iteration 10, loss = 0.14184125\n",
      "Iteration 11, loss = 0.13158600\n",
      "Iteration 12, loss = 0.19885006\n",
      "Iteration 13, loss = 0.12853187\n",
      "Iteration 14, loss = 0.11486160\n",
      "Iteration 15, loss = 0.10521254\n",
      "Iteration 16, loss = 0.09750412\n",
      "Iteration 17, loss = 0.09179422\n",
      "Iteration 18, loss = 0.08654770\n",
      "Iteration 19, loss = 0.08130566\n",
      "Iteration 20, loss = 0.07659505\n",
      "Iteration 21, loss = 0.07236283\n",
      "Iteration 22, loss = 0.06841583\n",
      "Iteration 23, loss = 0.06648973\n",
      "Iteration 24, loss = 0.06215475\n",
      "Iteration 25, loss = 0.05870745\n",
      "Iteration 26, loss = 0.05637246\n",
      "Iteration 27, loss = 0.05372698\n",
      "Iteration 28, loss = 0.05186450\n",
      "Iteration 29, loss = 0.04886849\n",
      "Iteration 30, loss = 0.04696263\n",
      "Iteration 31, loss = 0.04476903\n",
      "Iteration 32, loss = 0.04265315\n",
      "Iteration 33, loss = 0.04128399\n",
      "Iteration 34, loss = 0.03928033\n",
      "Iteration 35, loss = 0.03818311\n",
      "Iteration 36, loss = 0.03646022\n",
      "Iteration 37, loss = 0.03483765\n",
      "Iteration 38, loss = 0.03331106\n",
      "Iteration 39, loss = 0.03190652\n",
      "Iteration 40, loss = 0.03037388\n",
      "Iteration 41, loss = 0.02947037\n",
      "Iteration 42, loss = 0.02842729\n",
      "Iteration 43, loss = 0.02721365\n",
      "Iteration 44, loss = 0.02638287\n",
      "Iteration 45, loss = 0.02523927\n",
      "Iteration 46, loss = 0.02422738\n",
      "Iteration 47, loss = 0.02354195\n",
      "Iteration 48, loss = 0.02288583\n",
      "Iteration 49, loss = 0.02161084\n",
      "Iteration 50, loss = 0.02103186\n",
      "Iteration 51, loss = 0.02023507\n",
      "Iteration 52, loss = 0.01950569\n",
      "Iteration 53, loss = 0.01887968\n",
      "Iteration 54, loss = 0.01838661\n",
      "Iteration 55, loss = 0.01775861\n",
      "Iteration 56, loss = 0.01721139\n",
      "Iteration 57, loss = 0.01668584\n",
      "Iteration 58, loss = 0.01618210\n",
      "Iteration 59, loss = 0.01576144\n",
      "Iteration 60, loss = 0.01527689\n",
      "Iteration 61, loss = 0.01487332\n",
      "Iteration 62, loss = 0.01440506\n",
      "Iteration 63, loss = 0.01508550\n",
      "Iteration 64, loss = 0.01391983\n",
      "Iteration 65, loss = 0.01337963\n",
      "Iteration 66, loss = 0.01446954\n",
      "Iteration 67, loss = 0.01295619\n",
      "Iteration 68, loss = 0.01251628\n",
      "Iteration 69, loss = 0.01216570\n",
      "Iteration 70, loss = 0.01834452\n",
      "Iteration 71, loss = 0.01259922\n",
      "Iteration 72, loss = 0.01175377\n",
      "Iteration 73, loss = 0.01135822\n",
      "Iteration 74, loss = 0.01104532\n",
      "Iteration 75, loss = 0.01081431\n",
      "Iteration 76, loss = 0.01057656\n",
      "Iteration 77, loss = 0.01108004\n",
      "Iteration 78, loss = 0.01071223\n",
      "Iteration 79, loss = 0.01015237\n",
      "Iteration 80, loss = 0.00988345\n",
      "Iteration 81, loss = 0.00965650\n",
      "Iteration 82, loss = 0.00955776\n",
      "Iteration 83, loss = 0.00956989\n",
      "Iteration 84, loss = 0.00923530\n",
      "Iteration 85, loss = 0.00906285\n",
      "Iteration 86, loss = 0.00893693\n",
      "Iteration 87, loss = 0.00883744\n",
      "Iteration 88, loss = 0.00873891\n",
      "Iteration 89, loss = 0.00854148\n",
      "Iteration 90, loss = 0.00843505\n",
      "Iteration 91, loss = 0.00837669\n",
      "Iteration 92, loss = 0.00824521\n",
      "Iteration 93, loss = 0.00816354\n",
      "Iteration 94, loss = 0.00808750\n",
      "Iteration 95, loss = 0.00795514\n",
      "Iteration 96, loss = 0.00791041\n",
      "Iteration 97, loss = 0.00778951\n",
      "Iteration 98, loss = 0.00775662\n",
      "Iteration 99, loss = 0.00767994\n",
      "Iteration 100, loss = 0.00757659\n",
      "Iteration 101, loss = 0.00747810\n",
      "Iteration 102, loss = 0.00742459\n",
      "Iteration 103, loss = 0.03942635\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82637030\n",
      "Iteration 2, loss = 0.47945270\n",
      "Iteration 3, loss = 0.33327192\n",
      "Iteration 4, loss = 0.27614998\n",
      "Iteration 5, loss = 0.23674519\n",
      "Iteration 6, loss = 0.20852192\n",
      "Iteration 7, loss = 0.18991206\n",
      "Iteration 8, loss = 0.16913378\n",
      "Iteration 9, loss = 0.15489069\n",
      "Iteration 10, loss = 0.14206900\n",
      "Iteration 11, loss = 0.13200747\n",
      "Iteration 12, loss = 0.19466785\n",
      "Iteration 13, loss = 0.12612181\n",
      "Iteration 14, loss = 0.11309530\n",
      "Iteration 15, loss = 0.10374932\n",
      "Iteration 16, loss = 0.09667877\n",
      "Iteration 17, loss = 0.09065775\n",
      "Iteration 18, loss = 0.08570086\n",
      "Iteration 19, loss = 0.08072595\n",
      "Iteration 20, loss = 0.07599340\n",
      "Iteration 21, loss = 0.07114563\n",
      "Iteration 22, loss = 0.06741971\n",
      "Iteration 23, loss = 0.06456855\n",
      "Iteration 24, loss = 0.06065734\n",
      "Iteration 25, loss = 0.05734082\n",
      "Iteration 26, loss = 0.05459876\n",
      "Iteration 27, loss = 0.05176446\n",
      "Iteration 28, loss = 0.05269300\n",
      "Iteration 29, loss = 0.04712384\n",
      "Iteration 30, loss = 0.04472647\n",
      "Iteration 31, loss = 0.04304271\n",
      "Iteration 32, loss = 0.04047687\n",
      "Iteration 33, loss = 0.03904886\n",
      "Iteration 34, loss = 0.03699726\n",
      "Iteration 35, loss = 0.04933161\n",
      "Iteration 36, loss = 0.03675950\n",
      "Iteration 37, loss = 0.03384009\n",
      "Iteration 38, loss = 0.03168991\n",
      "Iteration 39, loss = 0.03001927\n",
      "Iteration 40, loss = 0.02857950\n",
      "Iteration 41, loss = 0.02731270\n",
      "Iteration 42, loss = 0.02655065\n",
      "Iteration 43, loss = 0.02525665\n",
      "Iteration 44, loss = 0.02426236\n",
      "Iteration 45, loss = 0.02323612\n",
      "Iteration 46, loss = 0.02239090\n",
      "Iteration 47, loss = 0.02147425\n",
      "Iteration 48, loss = 0.02080595\n",
      "Iteration 49, loss = 0.02520158\n",
      "Iteration 50, loss = 0.02019746\n",
      "Iteration 51, loss = 0.01887562\n",
      "Iteration 52, loss = 0.01796000\n",
      "Iteration 53, loss = 0.01731412\n",
      "Iteration 54, loss = 0.01697275\n",
      "Iteration 55, loss = 0.01619396\n",
      "Iteration 56, loss = 0.01567202\n",
      "Iteration 57, loss = 0.01512346\n",
      "Iteration 58, loss = 0.01469372\n",
      "Iteration 59, loss = 0.01429660\n",
      "Iteration 60, loss = 0.01389501\n",
      "Iteration 61, loss = 0.01343827\n",
      "Iteration 62, loss = 0.01304432\n",
      "Iteration 63, loss = 0.02203289\n",
      "Iteration 64, loss = 0.01370971\n",
      "Iteration 65, loss = 0.01270893\n",
      "Iteration 66, loss = 0.01208696\n",
      "Iteration 67, loss = 0.01170345\n",
      "Iteration 68, loss = 0.01138165\n",
      "Iteration 69, loss = 0.01108165\n",
      "Iteration 70, loss = 0.01103374\n",
      "Iteration 71, loss = 0.01058204\n",
      "Iteration 72, loss = 0.01094586\n",
      "Iteration 73, loss = 0.01027588\n",
      "Iteration 74, loss = 0.01004145\n",
      "Iteration 75, loss = 0.00975587\n",
      "Iteration 76, loss = 0.00958093\n",
      "Iteration 77, loss = 0.00939570\n",
      "Iteration 78, loss = 0.01067546\n",
      "Iteration 79, loss = 0.00937203\n",
      "Iteration 80, loss = 0.00904729\n",
      "Iteration 81, loss = 0.00887277\n",
      "Iteration 82, loss = 0.00872424\n",
      "Iteration 83, loss = 0.00858827\n",
      "Iteration 84, loss = 0.00842837\n",
      "Iteration 85, loss = 0.00830195\n",
      "Iteration 86, loss = 0.00818397\n",
      "Iteration 87, loss = 0.00809801\n",
      "Iteration 88, loss = 0.01034511\n",
      "Iteration 89, loss = 0.00830543\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82669531\n",
      "Iteration 2, loss = 0.48032986\n",
      "Iteration 3, loss = 0.34375564\n",
      "Iteration 4, loss = 0.27836504\n",
      "Iteration 5, loss = 0.24000620\n",
      "Iteration 6, loss = 0.21674490\n",
      "Iteration 7, loss = 0.19088108\n",
      "Iteration 8, loss = 0.19342874\n",
      "Iteration 9, loss = 0.16032992\n",
      "Iteration 10, loss = 0.14711280\n",
      "Iteration 11, loss = 0.13590343\n",
      "Iteration 12, loss = 0.12602109\n",
      "Iteration 13, loss = 0.11848831\n",
      "Iteration 14, loss = 0.10977078\n",
      "Iteration 15, loss = 0.10329492\n",
      "Iteration 16, loss = 0.09660798\n",
      "Iteration 17, loss = 0.09249947\n",
      "Iteration 18, loss = 0.08585716\n",
      "Iteration 19, loss = 0.08095841\n",
      "Iteration 20, loss = 0.07691810\n",
      "Iteration 21, loss = 0.07269159\n",
      "Iteration 22, loss = 0.09885116\n",
      "Iteration 23, loss = 0.08371568\n",
      "Iteration 24, loss = 0.06829300\n",
      "Iteration 25, loss = 0.06283404\n",
      "Iteration 26, loss = 0.08241154\n",
      "Iteration 27, loss = 0.05922085\n",
      "Iteration 28, loss = 0.06139370\n",
      "Iteration 29, loss = 0.05172988\n",
      "Iteration 30, loss = 0.04870853\n",
      "Iteration 31, loss = 0.04621296\n",
      "Iteration 32, loss = 0.04406665\n",
      "Iteration 33, loss = 0.04222133\n",
      "Iteration 34, loss = 0.03985723\n",
      "Iteration 35, loss = 0.03840852\n",
      "Iteration 36, loss = 0.03664979\n",
      "Iteration 37, loss = 0.03510506\n",
      "Iteration 38, loss = 0.03360805\n",
      "Iteration 39, loss = 0.03372640\n",
      "Iteration 40, loss = 0.03139593\n",
      "Iteration 41, loss = 0.03277073\n",
      "Iteration 42, loss = 0.02910820\n",
      "Iteration 43, loss = 0.02788852\n",
      "Iteration 44, loss = 0.02645782\n",
      "Iteration 45, loss = 0.02525576\n",
      "Iteration 46, loss = 0.02464985\n",
      "Iteration 47, loss = 0.02457970\n",
      "Iteration 48, loss = 0.02299979\n",
      "Iteration 49, loss = 0.02195767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.02136370\n",
      "Iteration 51, loss = 0.02038070\n",
      "Iteration 52, loss = 0.01992368\n",
      "Iteration 53, loss = 0.01997451\n",
      "Iteration 54, loss = 0.01875272\n",
      "Iteration 55, loss = 0.01896425\n",
      "Iteration 56, loss = 0.01774635\n",
      "Iteration 57, loss = 0.01724413\n",
      "Iteration 58, loss = 0.01653511\n",
      "Iteration 59, loss = 0.01602432\n",
      "Iteration 60, loss = 0.01546201\n",
      "Iteration 61, loss = 0.01511752\n",
      "Iteration 62, loss = 0.01477851\n",
      "Iteration 63, loss = 0.01427153\n",
      "Iteration 64, loss = 0.01399042\n",
      "Iteration 65, loss = 0.01367624\n",
      "Iteration 66, loss = 0.01325104\n",
      "Iteration 67, loss = 0.01296351\n",
      "Iteration 68, loss = 0.01266544\n",
      "Iteration 69, loss = 0.01240890\n",
      "Iteration 70, loss = 0.01216145\n",
      "Iteration 71, loss = 0.01188250\n",
      "Iteration 72, loss = 0.01411377\n",
      "Iteration 73, loss = 0.01184504\n",
      "Iteration 74, loss = 0.01142133\n",
      "Iteration 75, loss = 0.01122492\n",
      "Iteration 76, loss = 0.01077642\n",
      "Iteration 77, loss = 0.01056078\n",
      "Iteration 78, loss = 0.01031034\n",
      "Iteration 79, loss = 0.01013368\n",
      "Iteration 80, loss = 0.00994637\n",
      "Iteration 81, loss = 0.00977017\n",
      "Iteration 82, loss = 0.00958559\n",
      "Iteration 83, loss = 0.00938479\n",
      "Iteration 84, loss = 0.00946428\n",
      "Iteration 85, loss = 0.00921031\n",
      "Iteration 86, loss = 0.00901062\n",
      "Iteration 87, loss = 0.00885150\n",
      "Iteration 88, loss = 0.00872017\n",
      "Iteration 89, loss = 0.00856797\n",
      "Iteration 90, loss = 0.00848267\n",
      "Iteration 91, loss = 0.00829318\n",
      "Iteration 92, loss = 0.00831210\n",
      "Iteration 93, loss = 0.00814192\n",
      "Iteration 94, loss = 0.00804833\n",
      "Iteration 95, loss = 0.00795892\n",
      "Iteration 96, loss = 0.00785404\n",
      "Iteration 97, loss = 0.00773336\n",
      "Iteration 98, loss = 0.00767031\n",
      "Iteration 99, loss = 0.00757464\n",
      "Iteration 100, loss = 0.00750229\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82612676\n",
      "Iteration 2, loss = 0.48115111\n",
      "Iteration 3, loss = 0.34200284\n",
      "Iteration 4, loss = 0.28550382\n",
      "Iteration 5, loss = 0.23926161\n",
      "Iteration 6, loss = 0.21042491\n",
      "Iteration 7, loss = 0.19606792\n",
      "Iteration 8, loss = 0.17778247\n",
      "Iteration 9, loss = 0.15636525\n",
      "Iteration 10, loss = 0.16142885\n",
      "Iteration 11, loss = 0.13824982\n",
      "Iteration 12, loss = 0.12608578\n",
      "Iteration 13, loss = 0.11744817\n",
      "Iteration 14, loss = 0.11223095\n",
      "Iteration 15, loss = 0.10151249\n",
      "Iteration 16, loss = 0.09743918\n",
      "Iteration 17, loss = 0.09428694\n",
      "Iteration 18, loss = 0.08530177\n",
      "Iteration 19, loss = 0.09171530\n",
      "Iteration 20, loss = 0.07794816\n",
      "Iteration 21, loss = 0.07270996\n",
      "Iteration 22, loss = 0.06750294\n",
      "Iteration 23, loss = 0.06440776\n",
      "Iteration 24, loss = 0.07535163\n",
      "Iteration 25, loss = 0.06265640\n",
      "Iteration 26, loss = 0.05615894\n",
      "Iteration 27, loss = 0.05274249\n",
      "Iteration 28, loss = 0.05071577\n",
      "Iteration 29, loss = 0.05356884\n",
      "Iteration 30, loss = 0.04620086\n",
      "Iteration 31, loss = 0.04353976\n",
      "Iteration 32, loss = 0.04149379\n",
      "Iteration 33, loss = 0.03916235\n",
      "Iteration 34, loss = 0.03737547\n",
      "Iteration 35, loss = 0.06213786\n",
      "Iteration 36, loss = 0.04035378\n",
      "Iteration 37, loss = 0.03634340\n",
      "Iteration 38, loss = 0.03450900\n",
      "Iteration 39, loss = 0.03222169\n",
      "Iteration 40, loss = 0.03050141\n",
      "Iteration 41, loss = 0.02925815\n",
      "Iteration 42, loss = 0.02745674\n",
      "Iteration 43, loss = 0.02645229\n",
      "Iteration 44, loss = 0.02548558\n",
      "Iteration 45, loss = 0.02432607\n",
      "Iteration 46, loss = 0.02337746\n",
      "Iteration 47, loss = 0.02268225\n",
      "Iteration 48, loss = 0.02252176\n",
      "Iteration 49, loss = 0.02109185\n",
      "Iteration 50, loss = 0.02022706\n",
      "Iteration 51, loss = 0.01961354\n",
      "Iteration 52, loss = 0.01893640\n",
      "Iteration 53, loss = 0.01820281\n",
      "Iteration 54, loss = 0.01879783\n",
      "Iteration 55, loss = 0.01736823\n",
      "Iteration 56, loss = 0.01674140\n",
      "Iteration 57, loss = 0.01617119\n",
      "Iteration 58, loss = 0.01573554\n",
      "Iteration 59, loss = 0.01533947\n",
      "Iteration 60, loss = 0.01489333\n",
      "Iteration 61, loss = 0.01443379\n",
      "Iteration 62, loss = 0.01403161\n",
      "Iteration 63, loss = 0.01370063\n",
      "Iteration 64, loss = 0.01336635\n",
      "Iteration 65, loss = 0.01315943\n",
      "Iteration 66, loss = 0.01277541\n",
      "Iteration 67, loss = 0.01250172\n",
      "Iteration 68, loss = 0.01213656\n",
      "Iteration 69, loss = 0.01192168\n",
      "Iteration 70, loss = 0.01180643\n",
      "Iteration 71, loss = 0.01142324\n",
      "Iteration 72, loss = 0.01114208\n",
      "Iteration 73, loss = 0.01095867\n",
      "Iteration 74, loss = 0.01070939\n",
      "Iteration 75, loss = 0.01053146\n",
      "Iteration 76, loss = 0.01031746\n",
      "Iteration 77, loss = 0.01021994\n",
      "Iteration 78, loss = 0.00997309\n",
      "Iteration 79, loss = 0.00986790\n",
      "Iteration 80, loss = 0.00963414\n",
      "Iteration 81, loss = 0.00945595\n",
      "Iteration 82, loss = 0.00976898\n",
      "Iteration 83, loss = 0.00921934\n",
      "Iteration 84, loss = 0.00904922\n",
      "Iteration 85, loss = 0.00887289\n",
      "Iteration 86, loss = 0.00876631\n",
      "Iteration 87, loss = 0.00861834\n",
      "Iteration 88, loss = 0.00848457\n",
      "Iteration 89, loss = 0.00836533\n",
      "Iteration 90, loss = 0.00826812\n",
      "Iteration 91, loss = 0.00813207\n",
      "Iteration 92, loss = 0.00803271\n",
      "Iteration 93, loss = 0.00792935\n",
      "Iteration 94, loss = 0.00830815\n",
      "Iteration 95, loss = 0.00782991\n",
      "Iteration 96, loss = 0.00769464\n",
      "Iteration 97, loss = 0.00762534\n",
      "Iteration 98, loss = 0.00751616\n",
      "Iteration 99, loss = 0.00740836\n",
      "Iteration 100, loss = 0.00733632\n",
      "Iteration 101, loss = 0.00726197\n",
      "Iteration 102, loss = 0.00720906\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82824026\n",
      "Iteration 2, loss = 0.48115152\n",
      "Iteration 3, loss = 0.34172315\n",
      "Iteration 4, loss = 0.28386624\n",
      "Iteration 5, loss = 0.23944265\n",
      "Iteration 6, loss = 0.21036439\n",
      "Iteration 7, loss = 0.19601253\n",
      "Iteration 8, loss = 0.17696242\n",
      "Iteration 9, loss = 0.15753127\n",
      "Iteration 10, loss = 0.16311657\n",
      "Iteration 11, loss = 0.13630107\n",
      "Iteration 12, loss = 0.12521930\n",
      "Iteration 13, loss = 0.11703243\n",
      "Iteration 14, loss = 0.11647953\n",
      "Iteration 15, loss = 0.10331572\n",
      "Iteration 16, loss = 0.09828196\n",
      "Iteration 17, loss = 0.09130479\n",
      "Iteration 18, loss = 0.08632946\n",
      "Iteration 19, loss = 0.10136799\n",
      "Iteration 20, loss = 0.08017179\n",
      "Iteration 21, loss = 0.07516737\n",
      "Iteration 22, loss = 0.07038909\n",
      "Iteration 23, loss = 0.06710211\n",
      "Iteration 24, loss = 0.06836863\n",
      "Iteration 25, loss = 0.06134416\n",
      "Iteration 26, loss = 0.05764009\n",
      "Iteration 27, loss = 0.05494799\n",
      "Iteration 28, loss = 0.05261797\n",
      "Iteration 29, loss = 0.05578707\n",
      "Iteration 30, loss = 0.04852341\n",
      "Iteration 31, loss = 0.04596322\n",
      "Iteration 32, loss = 0.04356310\n",
      "Iteration 33, loss = 0.04153412\n",
      "Iteration 34, loss = 0.03954845\n",
      "Iteration 35, loss = 0.05763069\n",
      "Iteration 36, loss = 0.03921529\n",
      "Iteration 37, loss = 0.03637774\n",
      "Iteration 38, loss = 0.03465457\n",
      "Iteration 39, loss = 0.03255012\n",
      "Iteration 40, loss = 0.03763076\n",
      "Iteration 41, loss = 0.03118560\n",
      "Iteration 42, loss = 0.02967113\n",
      "Iteration 43, loss = 0.02785569\n",
      "Iteration 44, loss = 0.02664050\n",
      "Iteration 45, loss = 0.02535844\n",
      "Iteration 46, loss = 0.02453479\n",
      "Iteration 47, loss = 0.02379165\n",
      "Iteration 48, loss = 0.02261031\n",
      "Iteration 49, loss = 0.02170593\n",
      "Iteration 50, loss = 0.02090774\n",
      "Iteration 51, loss = 0.02022957\n",
      "Iteration 52, loss = 0.01946305\n",
      "Iteration 53, loss = 0.01884692\n",
      "Iteration 54, loss = 0.02343142\n",
      "Iteration 55, loss = 0.01856766\n",
      "Iteration 56, loss = 0.01767989\n",
      "Iteration 57, loss = 0.01682656\n",
      "Iteration 58, loss = 0.01633485\n",
      "Iteration 59, loss = 0.01574930\n",
      "Iteration 60, loss = 0.01517977\n",
      "Iteration 61, loss = 0.01477667\n",
      "Iteration 62, loss = 0.01445219\n",
      "Iteration 63, loss = 0.01397640\n",
      "Iteration 64, loss = 0.01369617\n",
      "Iteration 65, loss = 0.01328364\n",
      "Iteration 66, loss = 0.01295981\n",
      "Iteration 67, loss = 0.01263075\n",
      "Iteration 68, loss = 0.01237788\n",
      "Iteration 69, loss = 0.01206818\n",
      "Iteration 70, loss = 0.01393659\n",
      "Iteration 71, loss = 0.01196262\n",
      "Iteration 72, loss = 0.01195506\n",
      "Iteration 73, loss = 0.01129799\n",
      "Iteration 74, loss = 0.01086612\n",
      "Iteration 75, loss = 0.01070107\n",
      "Iteration 76, loss = 0.01037411\n",
      "Iteration 77, loss = 0.01020750\n",
      "Iteration 78, loss = 0.00999273\n",
      "Iteration 79, loss = 0.00976134\n",
      "Iteration 80, loss = 0.00962001\n",
      "Iteration 81, loss = 0.00936735\n",
      "Iteration 82, loss = 0.00925456\n",
      "Iteration 83, loss = 0.00911813\n",
      "Iteration 84, loss = 0.00897590\n",
      "Iteration 85, loss = 0.00881431\n",
      "Iteration 86, loss = 0.00869082\n",
      "Iteration 87, loss = 0.00853611\n",
      "Iteration 88, loss = 0.00848717\n",
      "Iteration 89, loss = 0.00831000\n",
      "Iteration 90, loss = 0.00822397\n",
      "Iteration 91, loss = 0.00812810\n",
      "Iteration 92, loss = 0.00800434\n",
      "Iteration 93, loss = 0.00788441\n",
      "Iteration 94, loss = 0.00783118\n",
      "Iteration 95, loss = 0.00770866\n",
      "Iteration 96, loss = 0.00761197\n",
      "Iteration 97, loss = 0.00755675\n",
      "Iteration 98, loss = 0.00746121\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81838709\n",
      "Iteration 2, loss = 0.43845026\n",
      "Iteration 3, loss = 0.38637382\n",
      "Iteration 4, loss = 0.37230464\n",
      "Iteration 5, loss = 0.36856635\n",
      "Iteration 6, loss = 0.36484592\n",
      "Iteration 7, loss = 0.35268733\n",
      "Iteration 8, loss = 0.35547022\n",
      "Iteration 9, loss = 0.35603123\n",
      "Iteration 10, loss = 0.34812799\n",
      "Iteration 11, loss = 0.35014525\n",
      "Iteration 12, loss = 0.35715827\n",
      "Iteration 13, loss = 0.35863038\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74508272\n",
      "Iteration 2, loss = 0.38902097\n",
      "Iteration 3, loss = 0.38130065\n",
      "Iteration 4, loss = 0.36303076\n",
      "Iteration 5, loss = 0.35417249\n",
      "Iteration 6, loss = 0.34969519\n",
      "Iteration 7, loss = 0.34727175\n",
      "Iteration 8, loss = 0.34051039\n",
      "Iteration 9, loss = 0.34527666\n",
      "Iteration 10, loss = 0.35212464\n",
      "Iteration 11, loss = 0.34908484\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25601123\n",
      "Iteration 2, loss = 0.65570071\n",
      "Iteration 3, loss = 0.52703951\n",
      "Iteration 4, loss = 0.44632114\n",
      "Iteration 5, loss = 0.41199984\n",
      "Iteration 6, loss = 0.39590081\n",
      "Iteration 7, loss = 0.39976314\n",
      "Iteration 8, loss = 0.38951320\n",
      "Iteration 9, loss = 0.38513537\n",
      "Iteration 10, loss = 0.38393096\n",
      "Iteration 11, loss = 0.38628446\n",
      "Iteration 12, loss = 0.37015463\n",
      "Iteration 13, loss = 0.37483259\n",
      "Iteration 14, loss = 0.37196269\n",
      "Iteration 15, loss = 0.37764963\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85212320\n",
      "Iteration 2, loss = 0.44685648\n",
      "Iteration 3, loss = 0.41028430\n",
      "Iteration 4, loss = 0.38969476\n",
      "Iteration 5, loss = 0.37171298\n",
      "Iteration 6, loss = 0.36913242\n",
      "Iteration 7, loss = 0.37060316\n",
      "Iteration 8, loss = 0.36708750\n",
      "Iteration 9, loss = 0.38306522\n",
      "Iteration 10, loss = 0.36977994\n",
      "Iteration 11, loss = 0.36691241\n",
      "Iteration 12, loss = 0.35617612\n",
      "Iteration 13, loss = 0.37509699\n",
      "Iteration 14, loss = 0.37155982\n",
      "Iteration 15, loss = 0.37577932\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86886488\n",
      "Iteration 2, loss = 0.44729661\n",
      "Iteration 3, loss = 0.39234458\n",
      "Iteration 4, loss = 0.37294458\n",
      "Iteration 5, loss = 0.36265569\n",
      "Iteration 6, loss = 0.36360707\n",
      "Iteration 7, loss = 0.35659481\n",
      "Iteration 8, loss = 0.34863049\n",
      "Iteration 9, loss = 0.35358510\n",
      "Iteration 10, loss = 0.35436694\n",
      "Iteration 11, loss = 0.35742965\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96631409\n",
      "Iteration 2, loss = 0.48062730\n",
      "Iteration 3, loss = 0.41829388\n",
      "Iteration 4, loss = 0.39289526\n",
      "Iteration 5, loss = 0.38522475\n",
      "Iteration 6, loss = 0.37159030\n",
      "Iteration 7, loss = 0.36690429\n",
      "Iteration 8, loss = 0.35023347\n",
      "Iteration 9, loss = 0.37117249\n",
      "Iteration 10, loss = 0.34884919\n",
      "Iteration 11, loss = 0.35245771\n",
      "Iteration 12, loss = 0.35537982\n",
      "Iteration 13, loss = 0.35438022\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81742882\n",
      "Iteration 2, loss = 0.38018965\n",
      "Iteration 3, loss = 0.35894042\n",
      "Iteration 4, loss = 0.34613667\n",
      "Iteration 5, loss = 0.33950898\n",
      "Iteration 6, loss = 0.33235034\n",
      "Iteration 7, loss = 0.34565972\n",
      "Iteration 8, loss = 0.33477189\n",
      "Iteration 9, loss = 0.32685265\n",
      "Iteration 10, loss = 0.32989341\n",
      "Iteration 11, loss = 0.32614118\n",
      "Iteration 12, loss = 0.35539043\n",
      "Iteration 13, loss = 0.36605690\n",
      "Iteration 14, loss = 0.34799402\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89009056\n",
      "Iteration 2, loss = 0.42649034\n",
      "Iteration 3, loss = 0.39870172\n",
      "Iteration 4, loss = 0.37519312\n",
      "Iteration 5, loss = 0.34935490\n",
      "Iteration 6, loss = 0.34546954\n",
      "Iteration 7, loss = 0.34602100\n",
      "Iteration 8, loss = 0.39007853\n",
      "Iteration 9, loss = 0.35213227\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79217084\n",
      "Iteration 2, loss = 0.40669269\n",
      "Iteration 3, loss = 0.39744262\n",
      "Iteration 4, loss = 0.38189410\n",
      "Iteration 5, loss = 0.35413666\n",
      "Iteration 6, loss = 0.35196103\n",
      "Iteration 7, loss = 0.34754612\n",
      "Iteration 8, loss = 0.35734690\n",
      "Iteration 9, loss = 0.35282742\n",
      "Iteration 10, loss = 0.36499286\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80921659\n",
      "Iteration 2, loss = 0.42112191\n",
      "Iteration 3, loss = 0.41522324\n",
      "Iteration 4, loss = 0.41310919\n",
      "Iteration 5, loss = 0.36434035\n",
      "Iteration 6, loss = 0.35499522\n",
      "Iteration 7, loss = 0.36709132\n",
      "Iteration 8, loss = 0.39272813\n",
      "Iteration 9, loss = 0.35784825\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82928619\n",
      "Iteration 2, loss = 0.48212031\n",
      "Iteration 3, loss = 0.33655717\n",
      "Iteration 4, loss = 0.27803906\n",
      "Iteration 5, loss = 0.23925086\n",
      "Iteration 6, loss = 0.21102638\n",
      "Iteration 7, loss = 0.18869712\n",
      "Iteration 8, loss = 0.17093405\n",
      "Iteration 9, loss = 0.15601727\n",
      "Iteration 10, loss = 0.14352893\n",
      "Iteration 11, loss = 0.13306124\n",
      "Iteration 12, loss = 0.12311761\n",
      "Iteration 13, loss = 0.11498943\n",
      "Iteration 14, loss = 0.10738989\n",
      "Iteration 15, loss = 0.10074006\n",
      "Iteration 16, loss = 0.09499653\n",
      "Iteration 17, loss = 0.08916001\n",
      "Iteration 18, loss = 0.08382761\n",
      "Iteration 19, loss = 0.07975600\n",
      "Iteration 20, loss = 0.07497576\n",
      "Iteration 21, loss = 0.07081077\n",
      "Iteration 22, loss = 0.06766468\n",
      "Iteration 23, loss = 0.06402305\n",
      "Iteration 24, loss = 0.06065137\n",
      "Iteration 25, loss = 0.05774831\n",
      "Iteration 26, loss = 0.05473864\n",
      "Iteration 27, loss = 0.05201944\n",
      "Iteration 28, loss = 0.04930189\n",
      "Iteration 29, loss = 0.04692823\n",
      "Iteration 30, loss = 0.04470820\n",
      "Iteration 31, loss = 0.04245649\n",
      "Iteration 32, loss = 0.04068296\n",
      "Iteration 33, loss = 0.03881975\n",
      "Iteration 34, loss = 0.03719346\n",
      "Iteration 35, loss = 0.03520797\n",
      "Iteration 36, loss = 0.03382177\n",
      "Iteration 37, loss = 0.03215398\n",
      "Iteration 38, loss = 0.03094792\n",
      "Iteration 39, loss = 0.02958308\n",
      "Iteration 40, loss = 0.02831121\n",
      "Iteration 41, loss = 0.02682679\n",
      "Iteration 42, loss = 0.02578095\n",
      "Iteration 43, loss = 0.02470842\n",
      "Iteration 44, loss = 0.02392849\n",
      "Iteration 45, loss = 0.02292626\n",
      "Iteration 46, loss = 0.02171276\n",
      "Iteration 47, loss = 0.02092689\n",
      "Iteration 48, loss = 0.02006036\n",
      "Iteration 49, loss = 0.01950749\n",
      "Iteration 50, loss = 0.01855306\n",
      "Iteration 51, loss = 0.01814950\n",
      "Iteration 52, loss = 0.01727156\n",
      "Iteration 53, loss = 0.01653072\n",
      "Iteration 54, loss = 0.01602818\n",
      "Iteration 55, loss = 0.01537990\n",
      "Iteration 56, loss = 0.01485211\n",
      "Iteration 57, loss = 0.01424917\n",
      "Iteration 58, loss = 0.01378031\n",
      "Iteration 59, loss = 0.01334161\n",
      "Iteration 60, loss = 0.01289304\n",
      "Iteration 61, loss = 0.01254982\n",
      "Iteration 62, loss = 0.01198346\n",
      "Iteration 63, loss = 0.01164503\n",
      "Iteration 64, loss = 0.01131020\n",
      "Iteration 65, loss = 0.01092622\n",
      "Iteration 66, loss = 0.01057978\n",
      "Iteration 67, loss = 0.01033729\n",
      "Iteration 68, loss = 0.00995760\n",
      "Iteration 69, loss = 0.00968498\n",
      "Iteration 70, loss = 0.00947073\n",
      "Iteration 71, loss = 0.00922359\n",
      "Iteration 72, loss = 0.00901942\n",
      "Iteration 73, loss = 0.00874508\n",
      "Iteration 74, loss = 0.00853603\n",
      "Iteration 75, loss = 0.00834241\n",
      "Iteration 76, loss = 0.00815892\n",
      "Iteration 77, loss = 0.00792492\n",
      "Iteration 78, loss = 0.00779308\n",
      "Iteration 79, loss = 0.00761248\n",
      "Iteration 80, loss = 0.00744391\n",
      "Iteration 81, loss = 0.00728394\n",
      "Iteration 82, loss = 0.00716572\n",
      "Iteration 83, loss = 0.00700246\n",
      "Iteration 84, loss = 0.00688499\n",
      "Iteration 85, loss = 0.00672670\n",
      "Iteration 86, loss = 0.00665012\n",
      "Iteration 87, loss = 0.00649430\n",
      "Iteration 88, loss = 0.00635850\n",
      "Iteration 89, loss = 0.00628217\n",
      "Iteration 90, loss = 0.00614042\n",
      "Iteration 91, loss = 0.00604968\n",
      "Iteration 92, loss = 0.00594160\n",
      "Iteration 93, loss = 0.00584377\n",
      "Iteration 94, loss = 0.00572916\n",
      "Iteration 95, loss = 0.00564985\n",
      "Iteration 96, loss = 0.00555962\n",
      "Iteration 97, loss = 0.00548043\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83116668\n",
      "Iteration 2, loss = 0.48085345\n",
      "Iteration 3, loss = 0.33416004\n",
      "Iteration 4, loss = 0.27677729\n",
      "Iteration 5, loss = 0.23790585\n",
      "Iteration 6, loss = 0.21030267\n",
      "Iteration 7, loss = 0.18950413\n",
      "Iteration 8, loss = 0.17154036\n",
      "Iteration 9, loss = 0.15598047\n",
      "Iteration 10, loss = 0.14302840\n",
      "Iteration 11, loss = 0.13218854\n",
      "Iteration 12, loss = 0.12278033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.11439010\n",
      "Iteration 14, loss = 0.10661669\n",
      "Iteration 15, loss = 0.09986518\n",
      "Iteration 16, loss = 0.09350034\n",
      "Iteration 17, loss = 0.08745663\n",
      "Iteration 18, loss = 0.08263094\n",
      "Iteration 19, loss = 0.07757184\n",
      "Iteration 20, loss = 0.07315301\n",
      "Iteration 21, loss = 0.06872381\n",
      "Iteration 22, loss = 0.06482814\n",
      "Iteration 23, loss = 0.06173172\n",
      "Iteration 24, loss = 0.05823930\n",
      "Iteration 25, loss = 0.05532162\n",
      "Iteration 26, loss = 0.05264212\n",
      "Iteration 27, loss = 0.05007449\n",
      "Iteration 28, loss = 0.04730352\n",
      "Iteration 29, loss = 0.04542154\n",
      "Iteration 30, loss = 0.04297498\n",
      "Iteration 31, loss = 0.04065992\n",
      "Iteration 32, loss = 0.03896951\n",
      "Iteration 33, loss = 0.03721920\n",
      "Iteration 34, loss = 0.03538147\n",
      "Iteration 35, loss = 0.03343367\n",
      "Iteration 36, loss = 0.03213772\n",
      "Iteration 37, loss = 0.03066129\n",
      "Iteration 38, loss = 0.02933797\n",
      "Iteration 39, loss = 0.02815275\n",
      "Iteration 40, loss = 0.02660146\n",
      "Iteration 41, loss = 0.02549768\n",
      "Iteration 42, loss = 0.02439151\n",
      "Iteration 43, loss = 0.02329721\n",
      "Iteration 44, loss = 0.02242693\n",
      "Iteration 45, loss = 0.02130389\n",
      "Iteration 46, loss = 0.02033330\n",
      "Iteration 47, loss = 0.01938822\n",
      "Iteration 48, loss = 0.01859460\n",
      "Iteration 49, loss = 0.01811136\n",
      "Iteration 50, loss = 0.01734792\n",
      "Iteration 51, loss = 0.01686311\n",
      "Iteration 52, loss = 0.01602842\n",
      "Iteration 53, loss = 0.01555527\n",
      "Iteration 54, loss = 0.01489718\n",
      "Iteration 55, loss = 0.01439936\n",
      "Iteration 56, loss = 0.01382098\n",
      "Iteration 57, loss = 0.01332479\n",
      "Iteration 58, loss = 0.01289097\n",
      "Iteration 59, loss = 0.01247476\n",
      "Iteration 60, loss = 0.01207057\n",
      "Iteration 61, loss = 0.01164127\n",
      "Iteration 62, loss = 0.01137770\n",
      "Iteration 63, loss = 0.01098760\n",
      "Iteration 64, loss = 0.01058533\n",
      "Iteration 65, loss = 0.01032058\n",
      "Iteration 66, loss = 0.00996216\n",
      "Iteration 67, loss = 0.00973270\n",
      "Iteration 68, loss = 0.00937633\n",
      "Iteration 69, loss = 0.00915286\n",
      "Iteration 70, loss = 0.00892739\n",
      "Iteration 71, loss = 0.00868343\n",
      "Iteration 72, loss = 0.00843514\n",
      "Iteration 73, loss = 0.00821231\n",
      "Iteration 74, loss = 0.00798572\n",
      "Iteration 75, loss = 0.00781382\n",
      "Iteration 76, loss = 0.00766672\n",
      "Iteration 77, loss = 0.00746566\n",
      "Iteration 78, loss = 0.00727717\n",
      "Iteration 79, loss = 0.00709985\n",
      "Iteration 80, loss = 0.00700194\n",
      "Iteration 81, loss = 0.00680838\n",
      "Iteration 82, loss = 0.00668357\n",
      "Iteration 83, loss = 0.00653870\n",
      "Iteration 84, loss = 0.00643135\n",
      "Iteration 85, loss = 0.00630139\n",
      "Iteration 86, loss = 0.00618061\n",
      "Iteration 87, loss = 0.00606090\n",
      "Iteration 88, loss = 0.00593205\n",
      "Iteration 89, loss = 0.00580733\n",
      "Iteration 90, loss = 0.00574349\n",
      "Iteration 91, loss = 0.00560643\n",
      "Iteration 92, loss = 0.00554528\n",
      "Iteration 93, loss = 0.00545423\n",
      "Iteration 94, loss = 0.00536329\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82908738\n",
      "Iteration 2, loss = 0.48163203\n",
      "Iteration 3, loss = 0.33834576\n",
      "Iteration 4, loss = 0.27833476\n",
      "Iteration 5, loss = 0.23977186\n",
      "Iteration 6, loss = 0.21146098\n",
      "Iteration 7, loss = 0.18876331\n",
      "Iteration 8, loss = 0.17111439\n",
      "Iteration 9, loss = 0.15681912\n",
      "Iteration 10, loss = 0.14376765\n",
      "Iteration 11, loss = 0.13228304\n",
      "Iteration 12, loss = 0.12288099\n",
      "Iteration 13, loss = 0.11431937\n",
      "Iteration 14, loss = 0.10655196\n",
      "Iteration 15, loss = 0.10001562\n",
      "Iteration 16, loss = 0.09394747\n",
      "Iteration 17, loss = 0.08876542\n",
      "Iteration 18, loss = 0.08329152\n",
      "Iteration 19, loss = 0.07851484\n",
      "Iteration 20, loss = 0.07401135\n",
      "Iteration 21, loss = 0.07045430\n",
      "Iteration 22, loss = 0.06656353\n",
      "Iteration 23, loss = 0.06260389\n",
      "Iteration 24, loss = 0.05966459\n",
      "Iteration 25, loss = 0.05650015\n",
      "Iteration 26, loss = 0.05370098\n",
      "Iteration 27, loss = 0.05121610\n",
      "Iteration 28, loss = 0.04862085\n",
      "Iteration 29, loss = 0.04600896\n",
      "Iteration 30, loss = 0.04398724\n",
      "Iteration 31, loss = 0.04126437\n",
      "Iteration 32, loss = 0.03966298\n",
      "Iteration 33, loss = 0.03756906\n",
      "Iteration 34, loss = 0.03609426\n",
      "Iteration 35, loss = 0.03430401\n",
      "Iteration 36, loss = 0.03264936\n",
      "Iteration 37, loss = 0.03126078\n",
      "Iteration 38, loss = 0.02977014\n",
      "Iteration 39, loss = 0.02819980\n",
      "Iteration 40, loss = 0.02692442\n",
      "Iteration 41, loss = 0.02549370\n",
      "Iteration 42, loss = 0.02458214\n",
      "Iteration 43, loss = 0.02346187\n",
      "Iteration 44, loss = 0.02254074\n",
      "Iteration 45, loss = 0.02135647\n",
      "Iteration 46, loss = 0.02057433\n",
      "Iteration 47, loss = 0.01951714\n",
      "Iteration 48, loss = 0.01867125\n",
      "Iteration 49, loss = 0.01811013\n",
      "Iteration 50, loss = 0.01728028\n",
      "Iteration 51, loss = 0.01655746\n",
      "Iteration 52, loss = 0.01598045\n",
      "Iteration 53, loss = 0.01538521\n",
      "Iteration 54, loss = 0.01479056\n",
      "Iteration 55, loss = 0.01418096\n",
      "Iteration 56, loss = 0.01359131\n",
      "Iteration 57, loss = 0.01323055\n",
      "Iteration 58, loss = 0.01268559\n",
      "Iteration 59, loss = 0.01234474\n",
      "Iteration 60, loss = 0.01188140\n",
      "Iteration 61, loss = 0.01147328\n",
      "Iteration 62, loss = 0.01114987\n",
      "Iteration 63, loss = 0.01080460\n",
      "Iteration 64, loss = 0.01039359\n",
      "Iteration 65, loss = 0.01013612\n",
      "Iteration 66, loss = 0.00986616\n",
      "Iteration 67, loss = 0.00955646\n",
      "Iteration 68, loss = 0.00930188\n",
      "Iteration 69, loss = 0.00907161\n",
      "Iteration 70, loss = 0.00881255\n",
      "Iteration 71, loss = 0.00859241\n",
      "Iteration 72, loss = 0.00835673\n",
      "Iteration 73, loss = 0.00816063\n",
      "Iteration 74, loss = 0.00800943\n",
      "Iteration 75, loss = 0.00776112\n",
      "Iteration 76, loss = 0.00758001\n",
      "Iteration 77, loss = 0.00742489\n",
      "Iteration 78, loss = 0.00727729\n",
      "Iteration 79, loss = 0.00712951\n",
      "Iteration 80, loss = 0.00695193\n",
      "Iteration 81, loss = 0.00683872\n",
      "Iteration 82, loss = 0.00670535\n",
      "Iteration 83, loss = 0.00655572\n",
      "Iteration 84, loss = 0.00645371\n",
      "Iteration 85, loss = 0.00629847\n",
      "Iteration 86, loss = 0.00619000\n",
      "Iteration 87, loss = 0.00606387\n",
      "Iteration 88, loss = 0.00597333\n",
      "Iteration 89, loss = 0.00586360\n",
      "Iteration 90, loss = 0.00577819\n",
      "Iteration 91, loss = 0.00565920\n",
      "Iteration 92, loss = 0.00557703\n",
      "Iteration 93, loss = 0.00549394\n",
      "Iteration 94, loss = 0.00539874\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82848366\n",
      "Iteration 2, loss = 0.47695383\n",
      "Iteration 3, loss = 0.33428248\n",
      "Iteration 4, loss = 0.27463311\n",
      "Iteration 5, loss = 0.23629508\n",
      "Iteration 6, loss = 0.20878537\n",
      "Iteration 7, loss = 0.18668677\n",
      "Iteration 8, loss = 0.16956637\n",
      "Iteration 9, loss = 0.15433440\n",
      "Iteration 10, loss = 0.14241479\n",
      "Iteration 11, loss = 0.13239302\n",
      "Iteration 12, loss = 0.12295964\n",
      "Iteration 13, loss = 0.11450344\n",
      "Iteration 14, loss = 0.10735914\n",
      "Iteration 15, loss = 0.09987617\n",
      "Iteration 16, loss = 0.09362035\n",
      "Iteration 17, loss = 0.08795154\n",
      "Iteration 18, loss = 0.08285063\n",
      "Iteration 19, loss = 0.07748467\n",
      "Iteration 20, loss = 0.07345133\n",
      "Iteration 21, loss = 0.06922778\n",
      "Iteration 22, loss = 0.06540830\n",
      "Iteration 23, loss = 0.06175999\n",
      "Iteration 24, loss = 0.05885181\n",
      "Iteration 25, loss = 0.05541059\n",
      "Iteration 26, loss = 0.05283693\n",
      "Iteration 27, loss = 0.05041428\n",
      "Iteration 28, loss = 0.04790564\n",
      "Iteration 29, loss = 0.04534496\n",
      "Iteration 30, loss = 0.04338460\n",
      "Iteration 31, loss = 0.04074278\n",
      "Iteration 32, loss = 0.03918318\n",
      "Iteration 33, loss = 0.03703223\n",
      "Iteration 34, loss = 0.03529932\n",
      "Iteration 35, loss = 0.03373405\n",
      "Iteration 36, loss = 0.03210328\n",
      "Iteration 37, loss = 0.03068767\n",
      "Iteration 38, loss = 0.02924340\n",
      "Iteration 39, loss = 0.02761618\n",
      "Iteration 40, loss = 0.02669623\n",
      "Iteration 41, loss = 0.02532446\n",
      "Iteration 42, loss = 0.02441057\n",
      "Iteration 43, loss = 0.02303407\n",
      "Iteration 44, loss = 0.02225446\n",
      "Iteration 45, loss = 0.02127769\n",
      "Iteration 46, loss = 0.02016357\n",
      "Iteration 47, loss = 0.01946803\n",
      "Iteration 48, loss = 0.01871727\n",
      "Iteration 49, loss = 0.01777041\n",
      "Iteration 50, loss = 0.01730671\n",
      "Iteration 51, loss = 0.01653117\n",
      "Iteration 52, loss = 0.01580108\n",
      "Iteration 53, loss = 0.01528241\n",
      "Iteration 54, loss = 0.01471373\n",
      "Iteration 55, loss = 0.01395202\n",
      "Iteration 56, loss = 0.01379954\n",
      "Iteration 57, loss = 0.01311384\n",
      "Iteration 58, loss = 0.01268811\n",
      "Iteration 59, loss = 0.01210150\n",
      "Iteration 60, loss = 0.01183482\n",
      "Iteration 61, loss = 0.01135321\n",
      "Iteration 62, loss = 0.01101457\n",
      "Iteration 63, loss = 0.01081927\n",
      "Iteration 64, loss = 0.01040837\n",
      "Iteration 65, loss = 0.01008705\n",
      "Iteration 66, loss = 0.00970615\n",
      "Iteration 67, loss = 0.00935541\n",
      "Iteration 68, loss = 0.00911527\n",
      "Iteration 69, loss = 0.00885615\n",
      "Iteration 70, loss = 0.00861816\n",
      "Iteration 71, loss = 0.00839997\n",
      "Iteration 72, loss = 0.00810386\n",
      "Iteration 73, loss = 0.00799294\n",
      "Iteration 74, loss = 0.00776816\n",
      "Iteration 75, loss = 0.00753751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.00736866\n",
      "Iteration 77, loss = 0.00723405\n",
      "Iteration 78, loss = 0.00702103\n",
      "Iteration 79, loss = 0.00688604\n",
      "Iteration 80, loss = 0.00672519\n",
      "Iteration 81, loss = 0.00656494\n",
      "Iteration 82, loss = 0.00644101\n",
      "Iteration 83, loss = 0.00632176\n",
      "Iteration 84, loss = 0.00617804\n",
      "Iteration 85, loss = 0.00609288\n",
      "Iteration 86, loss = 0.00595626\n",
      "Iteration 87, loss = 0.00584823\n",
      "Iteration 88, loss = 0.00574077\n",
      "Iteration 89, loss = 0.00563268\n",
      "Iteration 90, loss = 0.00551953\n",
      "Iteration 91, loss = 0.00544096\n",
      "Iteration 92, loss = 0.00535396\n",
      "Iteration 93, loss = 0.00528179\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82809225\n",
      "Iteration 2, loss = 0.48135617\n",
      "Iteration 3, loss = 0.33603201\n",
      "Iteration 4, loss = 0.27760767\n",
      "Iteration 5, loss = 0.23899098\n",
      "Iteration 6, loss = 0.21009744\n",
      "Iteration 7, loss = 0.18717446\n",
      "Iteration 8, loss = 0.16961267\n",
      "Iteration 9, loss = 0.15519675\n",
      "Iteration 10, loss = 0.14261404\n",
      "Iteration 11, loss = 0.13219272\n",
      "Iteration 12, loss = 0.12251889\n",
      "Iteration 13, loss = 0.11483583\n",
      "Iteration 14, loss = 0.10727893\n",
      "Iteration 15, loss = 0.10072092\n",
      "Iteration 16, loss = 0.09502586\n",
      "Iteration 17, loss = 0.08906123\n",
      "Iteration 18, loss = 0.08403275\n",
      "Iteration 19, loss = 0.07957701\n",
      "Iteration 20, loss = 0.07536099\n",
      "Iteration 21, loss = 0.07062728\n",
      "Iteration 22, loss = 0.06681864\n",
      "Iteration 23, loss = 0.06330912\n",
      "Iteration 24, loss = 0.06020821\n",
      "Iteration 25, loss = 0.05704573\n",
      "Iteration 26, loss = 0.05428566\n",
      "Iteration 27, loss = 0.05119841\n",
      "Iteration 28, loss = 0.04854637\n",
      "Iteration 29, loss = 0.04619039\n",
      "Iteration 30, loss = 0.04364970\n",
      "Iteration 31, loss = 0.04210575\n",
      "Iteration 32, loss = 0.03975821\n",
      "Iteration 33, loss = 0.03779887\n",
      "Iteration 34, loss = 0.03592639\n",
      "Iteration 35, loss = 0.03417480\n",
      "Iteration 36, loss = 0.03258317\n",
      "Iteration 37, loss = 0.03094001\n",
      "Iteration 38, loss = 0.02945905\n",
      "Iteration 39, loss = 0.02853730\n",
      "Iteration 40, loss = 0.02696747\n",
      "Iteration 41, loss = 0.02549653\n",
      "Iteration 42, loss = 0.02432342\n",
      "Iteration 43, loss = 0.02345493\n",
      "Iteration 44, loss = 0.02215608\n",
      "Iteration 45, loss = 0.02141092\n",
      "Iteration 46, loss = 0.02052346\n",
      "Iteration 47, loss = 0.01955593\n",
      "Iteration 48, loss = 0.01868980\n",
      "Iteration 49, loss = 0.01773918\n",
      "Iteration 50, loss = 0.01717417\n",
      "Iteration 51, loss = 0.01630371\n",
      "Iteration 52, loss = 0.01568244\n",
      "Iteration 53, loss = 0.01509386\n",
      "Iteration 54, loss = 0.01450145\n",
      "Iteration 55, loss = 0.01409017\n",
      "Iteration 56, loss = 0.01347309\n",
      "Iteration 57, loss = 0.01305644\n",
      "Iteration 58, loss = 0.01254960\n",
      "Iteration 59, loss = 0.01214041\n",
      "Iteration 60, loss = 0.01169755\n",
      "Iteration 61, loss = 0.01127634\n",
      "Iteration 62, loss = 0.01098527\n",
      "Iteration 63, loss = 0.01060280\n",
      "Iteration 64, loss = 0.01028033\n",
      "Iteration 65, loss = 0.00999679\n",
      "Iteration 66, loss = 0.00967667\n",
      "Iteration 67, loss = 0.00945165\n",
      "Iteration 68, loss = 0.00916655\n",
      "Iteration 69, loss = 0.00888143\n",
      "Iteration 70, loss = 0.00870894\n",
      "Iteration 71, loss = 0.00846656\n",
      "Iteration 72, loss = 0.00826106\n",
      "Iteration 73, loss = 0.00806106\n",
      "Iteration 74, loss = 0.00787481\n",
      "Iteration 75, loss = 0.00770918\n",
      "Iteration 76, loss = 0.00750321\n",
      "Iteration 77, loss = 0.00731944\n",
      "Iteration 78, loss = 0.00720011\n",
      "Iteration 79, loss = 0.00702257\n",
      "Iteration 80, loss = 0.00689411\n",
      "Iteration 81, loss = 0.00674125\n",
      "Iteration 82, loss = 0.00657918\n",
      "Iteration 83, loss = 0.00649349\n",
      "Iteration 84, loss = 0.00634725\n",
      "Iteration 85, loss = 0.00623573\n",
      "Iteration 86, loss = 0.00608091\n",
      "Iteration 87, loss = 0.00597100\n",
      "Iteration 88, loss = 0.00588514\n",
      "Iteration 89, loss = 0.00576740\n",
      "Iteration 90, loss = 0.00568833\n",
      "Iteration 91, loss = 0.00555996\n",
      "Iteration 92, loss = 0.00547159\n",
      "Iteration 93, loss = 0.00538901\n",
      "Iteration 94, loss = 0.00531382\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82714073\n",
      "Iteration 2, loss = 0.47683924\n",
      "Iteration 3, loss = 0.33199908\n",
      "Iteration 4, loss = 0.27456391\n",
      "Iteration 5, loss = 0.23535945\n",
      "Iteration 6, loss = 0.20679220\n",
      "Iteration 7, loss = 0.19076931\n",
      "Iteration 8, loss = 0.16800078\n",
      "Iteration 9, loss = 0.15346019\n",
      "Iteration 10, loss = 0.14111476\n",
      "Iteration 11, loss = 0.13081638\n",
      "Iteration 12, loss = 0.19807917\n",
      "Iteration 13, loss = 0.12773331\n",
      "Iteration 14, loss = 0.11400618\n",
      "Iteration 15, loss = 0.10431263\n",
      "Iteration 16, loss = 0.09656464\n",
      "Iteration 17, loss = 0.09081518\n",
      "Iteration 18, loss = 0.08552656\n",
      "Iteration 19, loss = 0.08024664\n",
      "Iteration 20, loss = 0.07550308\n",
      "Iteration 21, loss = 0.07124108\n",
      "Iteration 22, loss = 0.06726253\n",
      "Iteration 23, loss = 0.06526775\n",
      "Iteration 24, loss = 0.06093648\n",
      "Iteration 25, loss = 0.05744761\n",
      "Iteration 26, loss = 0.05508184\n",
      "Iteration 27, loss = 0.05241179\n",
      "Iteration 28, loss = 0.05046717\n",
      "Iteration 29, loss = 0.04748677\n",
      "Iteration 30, loss = 0.04555163\n",
      "Iteration 31, loss = 0.04333327\n",
      "Iteration 32, loss = 0.04118807\n",
      "Iteration 33, loss = 0.03978605\n",
      "Iteration 34, loss = 0.03775915\n",
      "Iteration 35, loss = 0.03660318\n",
      "Iteration 36, loss = 0.03488369\n",
      "Iteration 37, loss = 0.03324609\n",
      "Iteration 38, loss = 0.03169894\n",
      "Iteration 39, loss = 0.03028424\n",
      "Iteration 40, loss = 0.02872748\n",
      "Iteration 41, loss = 0.02779819\n",
      "Iteration 42, loss = 0.02674210\n",
      "Iteration 43, loss = 0.02550460\n",
      "Iteration 44, loss = 0.02465689\n",
      "Iteration 45, loss = 0.02350295\n",
      "Iteration 46, loss = 0.02246815\n",
      "Iteration 47, loss = 0.02177792\n",
      "Iteration 48, loss = 0.02109497\n",
      "Iteration 49, loss = 0.01982441\n",
      "Iteration 50, loss = 0.01922459\n",
      "Iteration 51, loss = 0.01841203\n",
      "Iteration 52, loss = 0.01766272\n",
      "Iteration 53, loss = 0.01703540\n",
      "Iteration 54, loss = 0.01651890\n",
      "Iteration 55, loss = 0.01587500\n",
      "Iteration 56, loss = 0.01531425\n",
      "Iteration 57, loss = 0.01478140\n",
      "Iteration 58, loss = 0.01427295\n",
      "Iteration 59, loss = 0.01384019\n",
      "Iteration 60, loss = 0.01335193\n",
      "Iteration 61, loss = 0.01293709\n",
      "Iteration 62, loss = 0.01246428\n",
      "Iteration 63, loss = 0.01301357\n",
      "Iteration 64, loss = 0.01193772\n",
      "Iteration 65, loss = 0.01141667\n",
      "Iteration 66, loss = 0.01230319\n",
      "Iteration 67, loss = 0.01095160\n",
      "Iteration 68, loss = 0.01052584\n",
      "Iteration 69, loss = 0.01018136\n",
      "Iteration 70, loss = 0.01817196\n",
      "Iteration 71, loss = 0.01087804\n",
      "Iteration 72, loss = 0.00988233\n",
      "Iteration 73, loss = 0.00942837\n",
      "Iteration 74, loss = 0.00909062\n",
      "Iteration 75, loss = 0.00883117\n",
      "Iteration 76, loss = 0.00858454\n",
      "Iteration 77, loss = 0.00901448\n",
      "Iteration 78, loss = 0.00855533\n",
      "Iteration 79, loss = 0.00810191\n",
      "Iteration 80, loss = 0.00784623\n",
      "Iteration 81, loss = 0.00762355\n",
      "Iteration 82, loss = 0.00751166\n",
      "Iteration 83, loss = 0.00746152\n",
      "Iteration 84, loss = 0.00717722\n",
      "Iteration 85, loss = 0.00701009\n",
      "Iteration 86, loss = 0.00688386\n",
      "Iteration 87, loss = 0.00677818\n",
      "Iteration 88, loss = 0.00666972\n",
      "Iteration 89, loss = 0.00648438\n",
      "Iteration 90, loss = 0.00637312\n",
      "Iteration 91, loss = 0.00630287\n",
      "Iteration 92, loss = 0.00617544\n",
      "Iteration 93, loss = 0.00608878\n",
      "Iteration 94, loss = 0.00600694\n",
      "Iteration 95, loss = 0.00587598\n",
      "Iteration 96, loss = 0.00582084\n",
      "Iteration 97, loss = 0.00570397\n",
      "Iteration 98, loss = 0.00566096\n",
      "Iteration 99, loss = 0.00558004\n",
      "Iteration 100, loss = 0.00547614\n",
      "Iteration 101, loss = 0.00537771\n",
      "Iteration 102, loss = 0.00531734\n",
      "Iteration 103, loss = 0.03086807\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82624965\n",
      "Iteration 2, loss = 0.47915145\n",
      "Iteration 3, loss = 0.33290878\n",
      "Iteration 4, loss = 0.27572265\n",
      "Iteration 5, loss = 0.23626160\n",
      "Iteration 6, loss = 0.20798546\n",
      "Iteration 7, loss = 0.18931064\n",
      "Iteration 8, loss = 0.16849831\n",
      "Iteration 9, loss = 0.15420800\n",
      "Iteration 10, loss = 0.14134338\n",
      "Iteration 11, loss = 0.13123865\n",
      "Iteration 12, loss = 0.19396558\n",
      "Iteration 13, loss = 0.12531582\n",
      "Iteration 14, loss = 0.11223726\n",
      "Iteration 15, loss = 0.10284786\n",
      "Iteration 16, loss = 0.09573556\n",
      "Iteration 17, loss = 0.08968156\n",
      "Iteration 18, loss = 0.08468210\n",
      "Iteration 19, loss = 0.07967239\n",
      "Iteration 20, loss = 0.07490726\n",
      "Iteration 21, loss = 0.07002970\n",
      "Iteration 22, loss = 0.06626995\n",
      "Iteration 23, loss = 0.06338145\n",
      "Iteration 24, loss = 0.05944197\n",
      "Iteration 25, loss = 0.05608768\n",
      "Iteration 26, loss = 0.05331653\n",
      "Iteration 27, loss = 0.05045041\n",
      "Iteration 28, loss = 0.05154632\n",
      "Iteration 29, loss = 0.04577859\n",
      "Iteration 30, loss = 0.04333852\n",
      "Iteration 31, loss = 0.04159205\n",
      "Iteration 32, loss = 0.03901507\n",
      "Iteration 33, loss = 0.03755288\n",
      "Iteration 34, loss = 0.03547758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.04775817\n",
      "Iteration 36, loss = 0.03517376\n",
      "Iteration 37, loss = 0.03225126\n",
      "Iteration 38, loss = 0.03006787\n",
      "Iteration 39, loss = 0.02837149\n",
      "Iteration 40, loss = 0.02692058\n",
      "Iteration 41, loss = 0.02561607\n",
      "Iteration 42, loss = 0.02484920\n",
      "Iteration 43, loss = 0.02354620\n",
      "Iteration 44, loss = 0.02251819\n",
      "Iteration 45, loss = 0.02147866\n",
      "Iteration 46, loss = 0.02062829\n",
      "Iteration 47, loss = 0.01969882\n",
      "Iteration 48, loss = 0.01899800\n",
      "Iteration 49, loss = 0.02452266\n",
      "Iteration 50, loss = 0.01857837\n",
      "Iteration 51, loss = 0.01712822\n",
      "Iteration 52, loss = 0.01617540\n",
      "Iteration 53, loss = 0.01549711\n",
      "Iteration 54, loss = 0.01510944\n",
      "Iteration 55, loss = 0.01435893\n",
      "Iteration 56, loss = 0.01381694\n",
      "Iteration 57, loss = 0.01326891\n",
      "Iteration 58, loss = 0.01282318\n",
      "Iteration 59, loss = 0.01241915\n",
      "Iteration 60, loss = 0.01201443\n",
      "Iteration 61, loss = 0.01155591\n",
      "Iteration 62, loss = 0.01115255\n",
      "Iteration 63, loss = 0.01974112\n",
      "Iteration 64, loss = 0.01174883\n",
      "Iteration 65, loss = 0.01078520\n",
      "Iteration 66, loss = 0.01016242\n",
      "Iteration 67, loss = 0.00978014\n",
      "Iteration 68, loss = 0.00945105\n",
      "Iteration 69, loss = 0.00915589\n",
      "Iteration 70, loss = 0.00903597\n",
      "Iteration 71, loss = 0.00863934\n",
      "Iteration 72, loss = 0.00888213\n",
      "Iteration 73, loss = 0.00830372\n",
      "Iteration 74, loss = 0.00806558\n",
      "Iteration 75, loss = 0.00779967\n",
      "Iteration 76, loss = 0.00761053\n",
      "Iteration 77, loss = 0.00742424\n",
      "Iteration 78, loss = 0.00857928\n",
      "Iteration 79, loss = 0.00736539\n",
      "Iteration 80, loss = 0.00706493\n",
      "Iteration 81, loss = 0.00689147\n",
      "Iteration 82, loss = 0.00672854\n",
      "Iteration 83, loss = 0.00659562\n",
      "Iteration 84, loss = 0.00643258\n",
      "Iteration 85, loss = 0.00629774\n",
      "Iteration 86, loss = 0.00618296\n",
      "Iteration 87, loss = 0.00608747\n",
      "Iteration 88, loss = 0.00771356\n",
      "Iteration 89, loss = 0.00618885\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82657461\n",
      "Iteration 2, loss = 0.48005724\n",
      "Iteration 3, loss = 0.34342314\n",
      "Iteration 4, loss = 0.27797265\n",
      "Iteration 5, loss = 0.23956412\n",
      "Iteration 6, loss = 0.21625594\n",
      "Iteration 7, loss = 0.19035023\n",
      "Iteration 8, loss = 0.19288740\n",
      "Iteration 9, loss = 0.15971631\n",
      "Iteration 10, loss = 0.14645950\n",
      "Iteration 11, loss = 0.13521169\n",
      "Iteration 12, loss = 0.12529190\n",
      "Iteration 13, loss = 0.11772161\n",
      "Iteration 14, loss = 0.10897457\n",
      "Iteration 15, loss = 0.10246583\n",
      "Iteration 16, loss = 0.09574974\n",
      "Iteration 17, loss = 0.09157267\n",
      "Iteration 18, loss = 0.08492894\n",
      "Iteration 19, loss = 0.08000034\n",
      "Iteration 20, loss = 0.07592832\n",
      "Iteration 21, loss = 0.07167820\n",
      "Iteration 22, loss = 0.09785658\n",
      "Iteration 23, loss = 0.08300201\n",
      "Iteration 24, loss = 0.06729344\n",
      "Iteration 25, loss = 0.06177130\n",
      "Iteration 26, loss = 0.08140294\n",
      "Iteration 27, loss = 0.05809983\n",
      "Iteration 28, loss = 0.06024138\n",
      "Iteration 29, loss = 0.05054125\n",
      "Iteration 30, loss = 0.04747811\n",
      "Iteration 31, loss = 0.04495122\n",
      "Iteration 32, loss = 0.04277322\n",
      "Iteration 33, loss = 0.04090784\n",
      "Iteration 34, loss = 0.03850148\n",
      "Iteration 35, loss = 0.03703487\n",
      "Iteration 36, loss = 0.03525638\n",
      "Iteration 37, loss = 0.03368748\n",
      "Iteration 38, loss = 0.03216512\n",
      "Iteration 39, loss = 0.03194395\n",
      "Iteration 40, loss = 0.02985197\n",
      "Iteration 41, loss = 0.03123850\n",
      "Iteration 42, loss = 0.02757500\n",
      "Iteration 43, loss = 0.02633808\n",
      "Iteration 44, loss = 0.02489770\n",
      "Iteration 45, loss = 0.02368356\n",
      "Iteration 46, loss = 0.02306259\n",
      "Iteration 47, loss = 0.02292429\n",
      "Iteration 48, loss = 0.02137191\n",
      "Iteration 49, loss = 0.02032691\n",
      "Iteration 50, loss = 0.01971468\n",
      "Iteration 51, loss = 0.01872527\n",
      "Iteration 52, loss = 0.01824717\n",
      "Iteration 53, loss = 0.01824682\n",
      "Iteration 54, loss = 0.01705681\n",
      "Iteration 55, loss = 0.01722403\n",
      "Iteration 56, loss = 0.01600031\n",
      "Iteration 57, loss = 0.01550418\n",
      "Iteration 58, loss = 0.01478583\n",
      "Iteration 59, loss = 0.01427452\n",
      "Iteration 60, loss = 0.01370472\n",
      "Iteration 61, loss = 0.01334986\n",
      "Iteration 62, loss = 0.01298229\n",
      "Iteration 63, loss = 0.01248538\n",
      "Iteration 64, loss = 0.01219718\n",
      "Iteration 65, loss = 0.01187833\n",
      "Iteration 66, loss = 0.01144531\n",
      "Iteration 67, loss = 0.01115042\n",
      "Iteration 68, loss = 0.01084672\n",
      "Iteration 69, loss = 0.01058601\n",
      "Iteration 70, loss = 0.01033070\n",
      "Iteration 71, loss = 0.01004183\n",
      "Iteration 72, loss = 0.01205419\n",
      "Iteration 73, loss = 0.00996103\n",
      "Iteration 74, loss = 0.00955545\n",
      "Iteration 75, loss = 0.00943429\n",
      "Iteration 76, loss = 0.00892595\n",
      "Iteration 77, loss = 0.00869266\n",
      "Iteration 78, loss = 0.00843536\n",
      "Iteration 79, loss = 0.00824950\n",
      "Iteration 80, loss = 0.00805686\n",
      "Iteration 81, loss = 0.00787059\n",
      "Iteration 82, loss = 0.00768026\n",
      "Iteration 83, loss = 0.00747310\n",
      "Iteration 84, loss = 0.00758726\n",
      "Iteration 85, loss = 0.00729242\n",
      "Iteration 86, loss = 0.00708276\n",
      "Iteration 87, loss = 0.00691800\n",
      "Iteration 88, loss = 0.00678223\n",
      "Iteration 89, loss = 0.00662314\n",
      "Iteration 90, loss = 0.00652788\n",
      "Iteration 91, loss = 0.00634102\n",
      "Iteration 92, loss = 0.00634834\n",
      "Iteration 93, loss = 0.00617864\n",
      "Iteration 94, loss = 0.00608064\n",
      "Iteration 95, loss = 0.00598175\n",
      "Iteration 96, loss = 0.00587603\n",
      "Iteration 97, loss = 0.00575353\n",
      "Iteration 98, loss = 0.00568563\n",
      "Iteration 99, loss = 0.00558658\n",
      "Iteration 100, loss = 0.00551053\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82600593\n",
      "Iteration 2, loss = 0.48088265\n",
      "Iteration 3, loss = 0.34167289\n",
      "Iteration 4, loss = 0.28511769\n",
      "Iteration 5, loss = 0.23882169\n",
      "Iteration 6, loss = 0.20994049\n",
      "Iteration 7, loss = 0.19555043\n",
      "Iteration 8, loss = 0.17721049\n",
      "Iteration 9, loss = 0.15575185\n",
      "Iteration 10, loss = 0.16079232\n",
      "Iteration 11, loss = 0.13755746\n",
      "Iteration 12, loss = 0.12534214\n",
      "Iteration 13, loss = 0.11668335\n",
      "Iteration 14, loss = 0.11137800\n",
      "Iteration 15, loss = 0.10068225\n",
      "Iteration 16, loss = 0.09657315\n",
      "Iteration 17, loss = 0.09336497\n",
      "Iteration 18, loss = 0.08438232\n",
      "Iteration 19, loss = 0.09075351\n",
      "Iteration 20, loss = 0.07698479\n",
      "Iteration 21, loss = 0.07171752\n",
      "Iteration 22, loss = 0.06647411\n",
      "Iteration 23, loss = 0.06334641\n",
      "Iteration 24, loss = 0.07426306\n",
      "Iteration 25, loss = 0.06158061\n",
      "Iteration 26, loss = 0.05501801\n",
      "Iteration 27, loss = 0.05156874\n",
      "Iteration 28, loss = 0.04950740\n",
      "Iteration 29, loss = 0.05226932\n",
      "Iteration 30, loss = 0.04493739\n",
      "Iteration 31, loss = 0.04225222\n",
      "Iteration 32, loss = 0.04017901\n",
      "Iteration 33, loss = 0.03782878\n",
      "Iteration 34, loss = 0.03601731\n",
      "Iteration 35, loss = 0.06085578\n",
      "Iteration 36, loss = 0.03896881\n",
      "Iteration 37, loss = 0.03493030\n",
      "Iteration 38, loss = 0.03306206\n",
      "Iteration 39, loss = 0.03075457\n",
      "Iteration 40, loss = 0.02901903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# nn = MLPClassifier(hidden_layer_sizes = (25,25,), solver='sgd', activation = 'logistic',learning_rate_init=0.01,learning_rate='invscaling')\n",
    "nn = MLPClassifier(hidden_layer_sizes=(50, 25),learning_rate_init=0.1,shuffle=True, max_iter=200,  verbose=True, tol=1e-4, random_state=1)\n",
    "\n",
    "parameter_grid = {'alpha': [1e-3, 3e-4, 1e-4, 3e-3, 1e-2],\n",
    "                  'activation': ['logistic','indentity'],\n",
    "                  'solver': ['sgd', 'adam']  \n",
    "                 }\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits= 10 )\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(nn,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "\n",
    "grid_search.fit(X_, y)\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn.fit(X_[0:30000],y[0:30000])\n",
    "\n",
    "# train_predict = nn.predict(X_)\n",
    "\n",
    "# print(train_predict)\n",
    "\n",
    "# test_predict = nn.predict(pca.transform(images_t))\n",
    "# print(test_predict[10])\n",
    "\n",
    "# print(nn.score(X_[0:30000],y[0:30000]))\n",
    "# print(nn.score(X_[30001:41999],y[30001:41999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_visualization = []\n",
    "\n",
    "for grid_pair in grid_search.grid_scores_:\n",
    "    grid_visualization.append(grid_pair.mean_validation_score)\n",
    "    \n",
    "grid_visualization = np.array(grid_visualization)\n",
    "grid_visualization.shape = (5, 4)\n",
    "sb.heatmap(grid_visualization, cmap='Blues')\n",
    "plt.xticks(np.arange(4) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(5) + 0.5, grid_search.param_grid['max_depth'][::-1])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in range(1,test_predict.shape[0]+1):\n",
    "    output.append([(i),(test_predict[i-1])])\n",
    "    \n",
    "# print(output)\n",
    "\n",
    "# np.insert(output,[0,0],[\"ImageId\", \"Label\"])\n",
    "print(output)\n",
    "\n",
    "# output.to_csv(\"trial1.csv\")delimiter=\n",
    "np.savetxt(\"trial1.csv\",output,delimiter=',', fmt='%d', header='\\b\\bImageId,Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = nn.coefs_[0].min(), nn.coefs_[0].max()\n",
    "for coef, ax in zip(nn.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
